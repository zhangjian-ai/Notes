## æ¦‚è¿°

LangChainæ˜¯ 2022å¹´10æœˆ ï¼Œç”±å“ˆä½›å¤§å­¦çš„ Harrison Chase ï¼ˆå“ˆé‡Œæ£®Â·è”¡æ–¯ï¼‰å‘èµ·ç ”å‘çš„ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œ ç”¨äºå¼€å‘ç”±å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é©±åŠ¨çš„åº”ç”¨ç¨‹åºã€‚é¡¹ç›®åœ°å€å¦‚ä¸‹ï¼š

 ```shell
https://github.com/langchain-ai/langchain
 ```

é¡¾åæ€ä¹‰ï¼ŒLangChainä¸­çš„â€œLangâ€æ˜¯æŒ‡languageï¼Œå³â¼¤è¯­â¾”æ¨¡å‹ï¼Œâ€œChainâ€å³â€œé“¾â€ï¼Œä¹Ÿå°±æ˜¯å°†â¼¤æ¨¡å‹ä¸å¤–éƒ¨æ•°æ®&å„ç§ç»„ä»¶è¿æ¥æˆé“¾ï¼Œä»¥æ­¤æ„å»ºAIåº”â½¤ç¨‹åºã€‚LangChain ä¹‹äº LLMsï¼Œç±»ä¼¼ Spring ä¹‹äº Java ï¼›Djangoã€Flask ä¹‹äº Pythonã€‚



### **å†…éƒ¨æ¶æ„**

#### ç»“æ„1ï¼šLangChain 

- langchainï¼šæ„æˆåº”ç”¨ç¨‹åºè®¤çŸ¥æ¶æ„çš„Chainsï¼ŒAgentsï¼ŒRetrieval strategiesç­‰ 
- langchain-communityï¼šç¬¬ä¸‰æ–¹é›†æˆã€‚æ¯”å¦‚ï¼šModel I/Oã€Retrievalã€Tool & Toolkitï¼›åˆä½œä¼™ä¼´åŒ… langchain-openaiï¼Œlangchainï¼Œanthropicç­‰ã€‚ 
- langchain-Coreï¼šåŸºç¡€æŠ½è±¡å’ŒLangChainè¡¨è¾¾å¼è¯­è¨€ (LCEL) 

å°ç»“ï¼šLangChainï¼Œå°±æ˜¯AIåº”ç”¨ç»„è£…å¥—ä»¶ï¼Œå°è£…äº†ä¸€å †çš„APIã€‚langchainæ¡†æ¶ä¸å¤§ï¼Œä½†æ˜¯é‡Œé¢çç¢çš„çŸ¥è¯†ç‚¹ç‰¹åˆ«å¤šã€‚å°±åƒç©ä¹é«˜ï¼Œæä¾›äº†å¾ˆå¤šæ ‡å‡†åŒ–çš„ä¹é«˜é›¶ä»¶ï¼ˆæ¯”å¦‚ï¼Œè¿æ¥å™¨ã€è½®å­ç­‰ï¼‰ 



#### ç»“æ„2ï¼šLangGraph 

LangGraphå¯ä»¥çœ‹åšåŸºäºLangChainçš„apiçš„è¿›ä¸€æ­¥å°è£…ï¼Œèƒ½å¤Ÿåè°ƒå¤šä¸ªChainã€Agentã€Toolså®Œæˆæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå®ç°æ›´é«˜çº§çš„åŠŸèƒ½ã€‚



#### ç»“æ„3ï¼šLangSmith 

> https://docs.smith.langchain.com/ 

é“¾è·¯è¿½è¸ªã€‚æä¾›äº†6å¤§åŠŸèƒ½ï¼Œæ¶‰åŠDebugging (è°ƒè¯•)ã€Playground (æ²™ç›’)ã€Prompt Management (æç¤ºç®¡ç†)ã€Annotation (æ³¨é‡Š)ã€Testing (æµ‹è¯•)ã€Monitoring (ç›‘æ§)ç­‰ã€‚ä¸LangChainæ— ç¼é›†æˆï¼Œå¸®åŠ©ä½ ä»åŸå‹é˜¶æ®µè¿‡æ¸¡åˆ°ç”Ÿäº§é˜¶æ®µã€‚  



#### ç»“æ„4ï¼šLangServe 

å°†LangChainçš„å¯è¿è¡Œé¡¹å’Œé“¾éƒ¨ç½²ä¸ºREST APIï¼Œä½¿å¾—å®ƒä»¬å¯ä»¥é€šè¿‡ç½‘ç»œè¿›è¡Œè°ƒç”¨ã€‚ Javaæ€ä¹ˆè°ƒç”¨langchainå‘¢ï¼Ÿå°±é€šè¿‡è¿™ä¸ªlangserveã€‚å°†langchainåº”ç”¨åŒ…è£…æˆä¸€ä¸ªrest apiï¼Œå¯¹å¤–æš´éœ²æœ åŠ¡ã€‚åŒæ—¶ï¼Œæ”¯æŒæ›´é«˜çš„å¹¶å‘ï¼Œç¨³å®šæ€§æ›´å¥½ã€‚



### åº”ç”¨å¼€å‘

> éœ€è¦å®‰è£…ä¾èµ–ï¼š
>
> pip install -i https://pypi.tuna.tsinghua.edu.cn/simple/ langchain dotenv

#### åŸºäºRAGæ¶æ„

**èƒŒæ™¯ï¼š **

- å¤§æ¨¡å‹çš„çŸ¥è¯†å†»ç»“ 
- å¤§æ¨¡å‹å¹»è§‰ 

è€ŒRAGå°±å¯ä»¥éå¸¸ç²¾å‡†çš„è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ã€‚ 



**ä¸¾ä¾‹ï¼š **

LLMåœ¨è€ƒè¯•çš„æ—¶å€™é¢å¯¹é™Œç”Ÿçš„é¢†åŸŸï¼Œç­”å¤èƒ½åŠ›æœ‰é™ï¼Œç„¶åå°±å‡†å¤‡æ”¾é£è‡ªæˆ‘äº†ã€‚è€Œæ­¤æ—¶RAGç»™äº†ä¸€äº›æç¤ºå’Œæ€è·¯ï¼Œè®©LLMæ‡‚äº†å¼€å§‹å¾€è¿™ä¸ªæç¤ºçš„æ–¹å‘åšï¼Œæœ€ç»ˆè€ƒè¯•çš„æ­£ç¡®ç‡ä»60%åˆ°äº†90%ï¼



**ä½•ä¸ºRAGï¼Ÿ**

Retrieval-Augmented Generation: æ£€ç´¢å¢å¼ºç”Ÿæˆã€‚ä¸‹å›¾ä¸­è“è‰²çš„æ­¥éª¤æ˜¯ç›¸å¯¹å¤æ‚çš„å¤„ç†æ­¥éª¤ã€‚

<img src='./images/001.png' style='width: 75%'>

**Rerankerä½¿ç”¨åœºæ™¯ï¼š**

- é€‚åˆï¼šè¿½æ±‚ **å›ç­”é«˜ç²¾åº¦** å’Œ **é«˜ç›¸å…³æ€§** çš„åœºæ™¯ä¸­ç‰¹åˆ«é€‚åˆä½¿ç”¨ Rerankerï¼Œä¾‹å¦‚ä¸“ä¸šçŸ¥è¯†åº“æˆ–è€…å®¢æœç³»ç»Ÿç­‰åº”ç”¨ã€‚
- ä¸é€‚åˆï¼šå¼•å…¥rerankerä¼šå¢åŠ å¬å›æ—¶é—´ï¼Œå¢åŠ æ£€ç´¢å»¶è¿Ÿã€‚æœåŠ¡å¯¹ **å“åº”æ—¶é—´è¦æ±‚é«˜** æ—¶ï¼Œä½¿ç”¨rerankerå¯èƒ½ä¸åˆé€‚ã€‚



**æ¶‰åŠåˆ°ä½¿ç”¨æ¨¡å‹çš„èŠ‚ç‚¹ï¼š**

1. ç¬¬3æ­¥å‘é‡åŒ–æ—¶ï¼Œéœ€è¦ä½¿ç”¨ EmbeddingModel
2. ç¬¬7æ­¥é‡æ’åºæ—¶ï¼Œéœ€è¦ä½¿ç”¨ RerankModel
3. ç¬¬9æ­¥ç”Ÿæˆç»“æœæ—¶ï¼Œéœ€è¦ä½¿ç”¨ LLM



#### åŸºäºAgentæ¶æ„

å……åˆ†åˆ©ç”¨ LLM çš„æ¨ç†å†³ç­–èƒ½åŠ›ï¼Œé€šè¿‡å¢åŠ  **è§„åˆ’** ã€ **è®°å¿†** å’Œ **å·¥å…·** è°ƒç”¨çš„èƒ½åŠ›ï¼Œæ„é€ ä¸€ä¸ªèƒ½å¤Ÿç‹¬ç«‹æ€è€ƒã€é€æ­¥å®Œæˆç»™å®šç›®æ ‡çš„æ™ºèƒ½ä½“ã€‚OpenAIçš„å…ƒè€ç¿ä¸½è²(Lilian Weng)äº2023å¹´6æœˆåœ¨ä¸ªäººåšå®¢é¦–æ¬¡æå‡ºäº†ç°ä»£AI Agentæ¶æ„ã€‚

<img src='./images/002.png' style='width: 75%'>

ä¸€ä¸ªæ•°å­¦å…¬å¼æ¥è¡¨ç¤ºï¼š**Agent = LLM + Memory + Tools + Planning + Action**

æ™ºèƒ½ä½“æ ¸å¿ƒè¦ç´ è¢«ç»†åŒ–ä¸ºä»¥ä¸‹æ¨¡å—ï¼š

1. **å¤§æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºâ€œå¤§è„‘â€**ï¼šæä¾›æ¨ç†ã€è§„åˆ’å’ŒçŸ¥è¯†ç†è§£èƒ½åŠ›ï¼Œæ˜¯AI Agentçš„å†³ç­–ä¸­æ¢ã€‚â¼¤è„‘ä¸»è¦ç”±â¼€ä¸ªâ¼¤å‹è¯­â¾”æ¨¡å‹ LLM ç»„æˆï¼Œæ‰¿æ‹…ç€ä¿¡æ¯å¤„ç†å’Œå†³ç­–ç­‰åŠŸèƒ½ï¼Œ å¹¶å¯ä»¥å‘ˆç°æ¨ç†å’Œè§„åˆ’çš„è¿‡ç¨‹ï¼Œèƒ½å¾ˆå¥½åœ°åº”å¯¹æœªçŸ¥ä»»åŠ¡ã€‚
2. **è®°å¿†ï¼ˆMemoryï¼‰**: è®°å¿†æœºåˆ¶èƒ½è®©æ™ºèƒ½ä½“åœ¨å¤„ç†é‡å¤â¼¯ä½œæ—¶è°ƒâ½¤ä»¥å‰çš„ç»éªŒï¼Œä»è€Œé¿å…â½¤â¼¾è¿›â¾â¼¤é‡é‡å¤äº¤äº’ã€‚
   - **çŸ­æœŸè®°å¿†**ï¼šå­˜å‚¨å•æ¬¡å¯¹è¯å‘¨æœŸçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå±äºä¸´æ—¶ä¿¡æ¯å­˜å‚¨æœºåˆ¶ã€‚å—é™äºæ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£é•¿åº¦ã€‚
   - **é•¿æœŸè®°å¿†**ï¼šå¯ä»¥æ¨ªè·¨å¤šä¸ªä»»åŠ¡æˆ–æ—¶é—´å‘¨æœŸï¼Œå¯å­˜å‚¨å¹¶è°ƒç”¨æ ¸å¿ƒçŸ¥è¯†ï¼Œéå³æ—¶ä»»åŠ¡ã€‚å¯ä»¥é€šè¿‡**æ¨¡å‹å‚æ•°å¾®è°ƒï¼ˆå›ºåŒ–çŸ¥è¯†ï¼‰**ã€**çŸ¥è¯†å›¾è°±ï¼ˆç»“æ„åŒ–è¯­ä¹‰ç½‘ç»œï¼‰**æˆ–**å‘é‡æ•°æ®åº“ï¼ˆç›¸ä¼¼æ€§æ£€ç´¢ï¼‰**æ–¹å¼å®ç°ã€‚
3. **å·¥å…·ä½¿ç”¨ï¼ˆTool Useï¼‰**ï¼šè°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚APIã€æ•°æ®åº“ï¼‰æ‰©å±•èƒ½åŠ›è¾¹ç•Œã€‚
4. **è§„åˆ’å†³ç­–ï¼ˆPlanningï¼‰**ï¼šé€šè¿‡ä»»åŠ¡åˆ†è§£ã€åæ€ä¸è‡ªçœæ¡†æ¶å®ç°å¤æ‚ä»»åŠ¡å¤„ç†ã€‚ä¾‹å¦‚ï¼Œåˆ©ç”¨æ€ç»´é“¾ï¼ˆChain of Thoughtï¼‰å°†ç›®æ ‡æ‹†è§£ä¸ºå­ä»»åŠ¡ï¼Œå¹¶é€šè¿‡åé¦ˆä¼˜åŒ–ç­–ç•¥ã€‚
5. **è¡ŒåŠ¨ï¼ˆActionï¼‰**ï¼šå®é™…æ‰§è¡Œå†³ç­–çš„æ¨¡å—ï¼Œæ¶µç›–è½¯ä»¶æ¥å£æ“ä½œï¼ˆå¦‚è‡ªåŠ¨è®¢ç¥¨ï¼‰å’Œç‰©ç†äº¤äº’ï¼ˆå¦‚æœºå™¨äººæ‰§è¡Œæ¬è¿ï¼‰ã€‚æ¯”å¦‚ï¼šæ£€ç´¢ã€æ¨ç†ã€ç¼–ç¨‹ç­‰ã€‚



#### åº”ç”¨å¼€å‘åœºæ™¯

**åœºæ™¯ä¸€ï¼šçº¯Prompt**

- Promptæ˜¯æ“ä½œå¤§æ¨¡å‹çš„å”¯ä¸€æ¥å£

- å½“äººçœ‹ï¼šä½ è¯´ä¸€å¥ï¼Œtaå›ä¸€å¥ï¼Œä½ å†è¯´ä¸€å¥ï¼Œtaå†å›ä¸€å¥...

<img src='./images/003.png' style='width: 20%'>



**åœºæ™¯äºŒï¼šAgent + Function Calling**

- Agentï¼šAI ä¸»åŠ¨æè¦æ±‚

- Function Callingï¼šéœ€è¦å¯¹æ¥å¤–éƒ¨ç³»ç»Ÿæ—¶ï¼ŒAI è¦æ±‚æ‰§è¡ŒæŸä¸ªå‡½æ•°

- å½“äººçœ‹ï¼šä½ é—® taã€Œæˆ‘æ˜å¤©å»æ­å·å‡ºå·®ï¼Œè¦å¸¦ä¼å—ï¼Ÿã€ï¼Œta è®©ä½ å…ˆçœ‹å¤©æ°”é¢„æŠ¥ï¼Œä½ çœ‹äº†å‘Šè¯‰taï¼Œtaå†å‘Šè¯‰ä½ è¦ä¸è¦å¸¦ä¼

<img src='./images/004.png' style='width: 40%'>



**åœºæ™¯ä¸‰ï¼šRAG (Retrieval-Augmented Generation)**

- RAGï¼šéœ€è¦è¡¥å……é¢†åŸŸçŸ¥è¯†æ—¶ä½¿ç”¨

- Embeddingsï¼šæŠŠæ–‡å­—è½¬æ¢ä¸ºæ›´æ˜“äºç›¸ä¼¼åº¦è®¡ç®—çš„ç¼–ç ã€‚è¿™ç§ç¼–ç å«å‘é‡

- å‘é‡æ•°æ®åº“ï¼šæŠŠå‘é‡å­˜èµ·æ¥ï¼Œæ–¹ä¾¿æŸ¥æ‰¾

- å‘é‡æœç´¢ï¼šæ ¹æ®è¾“å…¥å‘é‡ï¼Œæ‰¾åˆ°æœ€ç›¸ä¼¼çš„å‘é‡

ä¸¾ä¾‹ï¼šè€ƒè¯•ç­”é¢˜æ—¶ï¼Œåˆ°ä¹¦ä¸Šæ‰¾ç›¸å…³å†…å®¹ï¼Œå†ç»“åˆé¢˜ç›®ç»„æˆç­”æ¡ˆ

<img src='./images/005.png' style='width: 45%'>



**åœºæ™¯å››ï¼šFine-tuning(ç²¾è°ƒ/å¾®è°ƒ)**

ä¸¾ä¾‹ï¼šåŠªåŠ›å­¦ä¹ è€ƒè¯•å†…å®¹ï¼Œé•¿æœŸè®°ä½ï¼Œæ´»å­¦æ´»ç”¨ã€‚

<img src='./images/006.png' style='width: 45%'>

## Model I/O

Model I/O æ¨¡å—æ˜¯ä¸è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œäº¤äº’çš„ **æ ¸å¿ƒç»„ä»¶** ï¼Œåœ¨æ•´ä¸ªæ¡†æ¶ä¸­æœ‰ç€å¾ˆé‡è¦çš„åœ°ä½ã€‚æ‰€è°“çš„Model I/Oï¼ŒåŒ…æ‹¬è¾“å…¥æç¤º(Format)ã€è°ƒç”¨æ¨¡å‹(Predict)ã€è¾“å‡ºè§£æ(Parse)ã€‚åˆ†åˆ«å¯¹åº”ç€**Prompt Template**ï¼Œ**Model** å’Œ **Output Parser** ã€‚

é’ˆå¯¹æ¯ä¸ªç¯èŠ‚ï¼ŒLangChainéƒ½æä¾›äº†æ¨¡æ¿å’Œå·¥å…·ï¼Œå¯ä»¥å¿«æ·çš„è°ƒç”¨å„ç§è¯­è¨€æ¨¡å‹çš„æ¥å£ã€‚

<img src='./images/007.png' style='width: 75%'>

ç›®å‰OpenAIåœ¨å›½å†…æ˜¯ç¦æ­¢è®¿é—®çš„ï¼Œå¯ä»¥ä½¿ç”¨å›½å†…çš„ä»£ç†ç½‘ç«™è¿›è¡Œå……å€¼è°ƒç”¨ï¼š

https://www.closeai-asia.com

### æ¨¡å‹è°ƒç”¨

LangChainä½œä¸ºä¸€ä¸ªâ€œå·¥å…·â€ï¼Œä¸æä¾›ä»»ä½• LLMsï¼Œè€Œæ˜¯ä¾èµ–äºç¬¬ä¸‰æ–¹é›†æˆå„ç§å¤§æ¨¡å‹ã€‚æ¯”å¦‚ï¼Œå°†OpenAIã€Anthropicã€Hugging Face ã€LlaMAã€é˜¿é‡ŒQwenã€ChatGLMç­‰å¹³å°çš„æ¨¡å‹æ— ç¼æ¥å…¥åˆ°ä½ çš„åº”ç”¨ã€‚

#### æŒ‰åŠŸèƒ½åŒºåˆ†

- **LLMSï¼ˆéå¯¹è¯æ¨¡å‹ï¼‰**

  LLMsï¼Œä¹Ÿå«Text Modelã€éå¯¹è¯æ¨¡å‹ï¼Œæ˜¯è®¸å¤šè¯­è¨€æ¨¡å‹åº”ç”¨ç¨‹åºçš„æ”¯æŸ±ã€‚ä¸»è¦ç‰¹ç‚¹å¦‚ä¸‹ï¼š

  - **è¾“å…¥**ï¼šæ¥å— **æ–‡æœ¬å­—ç¬¦ä¸²** æˆ– **PromptValue** å¯¹è±¡
  - **è¾“å‡º**ï¼šæ€»æ˜¯è¿”å› **æ–‡æœ¬å­—ç¬¦ä¸²**
  - *é€‚ç”¨åœºæ™¯**ï¼šä»…éœ€å•æ¬¡æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚æ‘˜è¦ç”Ÿæˆã€ç¿»è¯‘ã€ä»£ç ç”Ÿæˆã€å•æ¬¡é—®ç­”ï¼‰æˆ–å¯¹æ¥ä¸æ”¯æŒæ¶ˆæ¯ç»“æ„çš„æ—§æ¨¡å‹ï¼ˆå¦‚éƒ¨åˆ†æœ¬åœ°éƒ¨ç½²æ¨¡å‹ï¼‰ï¼ˆ **è¨€å¤–ä¹‹æ„ï¼Œä¼˜å…ˆæ¨èChatModel** ï¼‰
  - **ä¸æ”¯æŒå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ï¼š**æ¯æ¬¡è°ƒç”¨ç‹¬ç«‹å¤„ç†è¾“å…¥ï¼Œæ— æ³•è‡ªåŠ¨å…³è”å†å²å¯¹è¯ï¼ˆéœ€æ‰‹åŠ¨æ‹¼æ¥å†å²æ–‡æœ¬ï¼‰ã€‚
  - **å±€é™æ€§**ï¼šæ— æ³•å¤„ç†è§’è‰²åˆ†å·¥æˆ–å¤æ‚å¯¹è¯é€»è¾‘

  ```python
  import os
  import dotenv
  from langchain_openai import OpenAI
  
  # å¯ä»¥åœ¨å½“å‰ç›®å½•åˆ›å»ºä¸€ä¸ª.envçš„æ–‡ä»¶é…ç½®å¥½API_KEYå’ŒBASE_URLç„¶åç›´æ¥åŠ è½½å³å¯
  dotenv.load_dotenv()
  
  # ä¹Ÿå¯ä»¥ç›´æ¥æ¥æ˜æ–‡é…ç½®
  # os.environ["OPENAI_API_KEY"] = "your api_key"
  # os.environ["OPENAI_BASE_URL"] = "your base_url"
  
  llm = OpenAI()  # å®ä¾‹åŒ–çš„æ—¶å€™ä¹Ÿå¯ä»¥ä¼ å…¥api_keyå’Œbase_urlçš„å‚æ•°
  result = llm.invoke("å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ï¼Œäº”è¨€ç»å¥")  # ç›´æ¥è¾“å…¥å­—ç¬¦ä¸²
  print(result)
  
  """
  æ˜¥è‡³äººé—´è¿èŠ±å¼€ï¼Œç‡•é£è¶èˆç»•ææ‘†ã€‚
  ä¸‡ç‰©å¤è‹ç”ŸæœºåŠ¨ï¼Œæ˜¥é£å¹æ‹‚æ·»è‰²å½©ã€‚
  """
  ```

- **Chat Models(å¯¹è¯æ¨¡å‹)**

  ChatModelsï¼Œä¹Ÿå«èŠå¤©æ¨¡å‹ã€å¯¹è¯æ¨¡å‹ï¼Œåº•å±‚ä½¿ç”¨LLMsã€‚ä¸»è¦ç‰¹ç‚¹å¦‚ä¸‹ï¼š

  - **è¾“å…¥**ï¼šæ¥æ”¶æ¶ˆæ¯åˆ—è¡¨ **List[BaseMessage]** æˆ– **PromptValue** ï¼Œæ¯æ¡æ¶ˆæ¯éœ€æŒ‡å®šè§’è‰²ï¼ˆå¦‚SystemMessageã€HumanMessageã€AIMessageï¼‰
  - **è¾“å‡º**ï¼šæ€»æ˜¯è¿”å›å¸¦è§’è‰²çš„ **æ¶ˆæ¯å¯¹è±¡** ï¼ˆ **BaseMessage** å­ç±»ï¼‰ï¼Œé€šå¸¸æ˜¯ **AIMessage**
  - **åŸç”Ÿæ”¯æŒå¤šè½®å¯¹è¯**ã€‚é€šè¿‡æ¶ˆæ¯åˆ—è¡¨ç»´æŠ¤ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚: [SystemMessage, HumanMessage,AIMessage, ...] ï¼‰ï¼Œæ¨¡å‹å¯åŸºäºå®Œæ•´å¯¹è¯å†å²ç”Ÿæˆå›å¤
  - é€‚ç”¨åœºæ™¯**ï¼šå¯¹è¯ç³»ç»Ÿï¼ˆå¦‚å®¢æœæœºå™¨äººã€é•¿æœŸäº¤äº’çš„AIåŠ©æ‰‹ï¼‰

  ```python
  import dotenv
  from langchain_core.messages import SystemMessage, HumanMessage
  from langchain_openai import ChatOpenAI
  
  dotenv.load_dotenv()
  
  llm = ChatOpenAI(model="gpt-4o-mini")
  
  # æ„å»ºæ¶ˆæ¯
  msg = [SystemMessage(content="æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæˆ‘å«ç¨€å®¢"),
         HumanMessage(content="ä½ å¥½æˆ‘æ˜¯å°å­¦ç”Ÿå°æ˜ï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ")]
  
  result = llm.invoke(msg)
  
  print(type(result))  # <class 'langchain_core.messages.ai.AIMessage'>
  print(result.content)  # ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
  ```

- **Embedding Model(åµŒå…¥æ¨¡å‹)**

  ä¹Ÿå«æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å°† **æ–‡æœ¬** ä½œä¸ºè¾“å…¥å¹¶è¿”å› **æµ®ç‚¹æ•°åˆ—è¡¨** ï¼Œä¹Ÿå°±æ˜¯Embeddingã€‚

  ```python
  import dotenv
  from langchain_openai import OpenAIEmbeddings
  
  dotenv.load_dotenv()
  
  # é…ç½®è¾“å‡ºå‘é‡çš„ç»´åº¦
  embedding = OpenAIEmbeddings(model="text-embedding-3-small", dimensions=12)
  
  result = embedding.embed_query(text="ä¸­å›½æ˜¯ä¸–ç•Œä¸Šé™†åœ°é¢ç§¯ç¬¬ä¸‰å¤§çš„å›½å®¶")
  
  print(result)
  """
  [0.25890520215034485, 0.0884762704372406, 0.4098992645740509,
   0.45529261231422424, 0.30769628286361694, -0.11062931269407272, 
   0.1800784319639206, 0.40527838468551636, -0.36477774381637573, 
   -0.2277822196483612, -0.20005694031715393, 0.14582954347133636]
  """
  ```

  

#### æŒ‰å¹³å°åŒºåˆ†

-  **OpenAI å®˜æ–¹API**

  - è°ƒç”¨éå¯¹è¯æ¨¡å‹

    ```python
    import dotenv
    
    from openai import OpenAI  # æ³¨æ„è¿™é‡Œå¯¼åŒ…å˜äº†
    
    dotenv.load_dotenv()
    
    client = OpenAI()
    
    # è°ƒç”¨completionæ¥å£
    result = client.completions.create(
        model="gpt-3.5-turbo-instruct",  # éå¯¹è¯æ¨¡å‹
        max_tokens=100,  # é™åˆ¶ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ï¼Œé˜²æ­¢è¾“å‡ºè¿‡é•¿
        temperature=0.7,  # æ§åˆ¶éšæœºæ€§ï¼Œå€¼è¶Šç‡è¶Šä¿å®ˆ
        prompt="ä½ å¥½ï¼Œè¯·é—®ä½ æ˜¯è°"
    )
    
    print(result.choices[0].text)
    """
    æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä¸ºæ‚¨æä¾›å¸®åŠ©å’Œè§£ç­”é—®é¢˜ã€‚æ‚¨æœ‰ä»€ä¹ˆéœ€è¦æˆ‘å¸®å¿™çš„å—ï¼Ÿ
    """
    ```

  - è°ƒç”¨å¯¹è¯æ¨¡å‹

    ```python
    import dotenv
    
    from openai import OpenAI  # æ³¨æ„è¿™é‡Œå¯¼åŒ…å˜äº†
    
    dotenv.load_dotenv()
    
    client = OpenAI()
    
    # æ³¨æ„è¿™é‡Œæ˜¯è°ƒç”¨ chat.completions
    result = client.chat.completions.create(
        model="gpt-3.5-turbo",  # å¯¹è¯æ¨¡å‹
        max_tokens=30,  # é™åˆ¶ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ï¼Œé˜²æ­¢è¾“å‡ºè¿‡é•¿
        temperature=0.7,  # æ§åˆ¶éšæœºæ€§ï¼Œå€¼è¶Šç‡è¶Šä¿å®ˆ
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„æ™ºèƒ½AIå°åŠ©æ‰‹"},
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
        ]
    )
    
    print(result.choices[0].message.content)
    """
    ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªæ™ºèƒ½AIå°åŠ©æ‰‹ï¼Œä¸“é—¨ç”¨äºå›ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯å’Œå¸®åŠ©è§£å†³å„ç§é—®é¢˜ã€‚ä¸è®ºä½ æ˜¯æƒ³äº†è§£çŸ¥è¯†ã€
    """
    ```

- **å›½å†…æ¨¡å‹å…¼å®¹**

  - ä½¿ç”¨openaiè°ƒç”¨deepseek

    ```python
    import os
    import dotenv
    
    from openai import OpenAI  # æ³¨æ„è¿™é‡Œå¯¼åŒ…å˜äº†
    
    # å·²åœ¨.envä¸­åŠ å…¥éœ€è¦çš„é…ç½®
    dotenv.load_dotenv()
    
    client = OpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"))
    
    # æ³¨æ„è¿™é‡Œæ˜¯è°ƒç”¨ chat.completions
    result = client.chat.completions.create(
        model=os.getenv("DS_MODEL"),  # å¯¹è¯æ¨¡å‹
        max_tokens=30,  # é™åˆ¶ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ï¼Œé˜²æ­¢è¾“å‡ºè¿‡é•¿
        temperature=0.7,  # æ§åˆ¶éšæœºæ€§ï¼Œå€¼è¶Šç‡è¶Šä¿å®ˆ
        messages=[
            {"role": "system", "content": "ä½ æ˜¯äººå·¥æ™ºèƒ½AIå°åŠ©æ‰‹"},
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
        ]
    )
    
    print(result.choices[0].message.content)
    """
    ä½ å¥½ï¼æˆ‘æ˜¯ **DeepSeek Chat**ï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸ï¼ˆDeepSeekï¼‰ç ”å‘çš„æ™ºèƒ½ AI åŠ©æ‰‹ã€‚æˆ‘å¯ä»¥å¸®åŠ©ä½ è§£ç­”
    """
    ```

  - ä½¿ç”¨langchainè°ƒç”¨åƒé—®

    ```python
    import os
    import dotenv
    
    from langchain.chains.llm import LLMChain
    from langchain.memory import ConversationBufferMemory
    from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder, \
        HumanMessagePromptTemplate
    
    from langchain_openai import ChatOpenAI
    
    dotenv.load_dotenv()
    
    # å®¢æˆ·ç«¯
    client = ChatOpenAI(api_key=os.getenv("QW_API_KEY"),
                        base_url=os.getenv("QW_BASE_URL"),
                        model=os.getenv("QW_MODEL"),
                        max_tokens=48)
    
    # prompt
    prompt = ChatPromptTemplate(
        messages=[
            SystemMessagePromptTemplate.from_template("ä½ æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½AIå°åŠ©æ‰‹"),
            MessagesPlaceholder(variable_name="chat_history"),  # ä»è®°å¿†å·¥å…·ä¸­ï¼Œé€šè¿‡ chat_history è¿™ä¸ªkeyè·å–æç¤ºä¸Šä¸‹æ–‡
            HumanMessagePromptTemplate.from_template("{question}",)  # å†™å…¥å ä½å˜é‡ï¼Œåç»­è°ƒç”¨ç»™è¿™ä¸ªkeyèµ‹å€¼
        ]
    )
    
    # è®°å¿†
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    
    # ä¼šè¯
    # verbose ä½œç”¨æ˜¾ç¤ºæ¨¡å‹è°ƒç”¨çš„è¯¦ç»†è¿‡ï¼ŒåŒ…æ‹¬è¾“å…¥è¾“å‡ºå†…å®¹å’Œä¸­é—´æ­¥éª¤ï¼Œä¾¿äºè°ƒè¯•å’Œè§‚å¯Ÿæ¨¡å‹è¿è¡ŒçŠ¶æ€ã€‚
    session = LLMChain(llm=client, prompt=prompt, memory=memory, verbose=True)
    
    # å¯¹è¯
    # è¿”å›ä¸€ä¸ªå­—å…¸å¯¹è±¡ï¼ŒåŒ…å«äº†é—®é¢˜ï¼Œå¯¹è¯å†å²ï¼Œä»¥åŠæœ¬è½®çš„ç»“æœ
    result = session.invoke({"question": "è¯·ä½ åšä¸€ä¸ªç®€å•çš„è‡ªæˆ‘ä»‹ç»"})
    
    print(result.keys())
    print(result.get("text"))  # ä½ å¥½ï¼æˆ‘æ˜¯Qwenï¼Œæ˜¯é˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„é€šä¹‰å®...
    print(result.get("question"))  # è¯·ä½ åšä¸€ä¸ªç®€å•çš„è‡ªæˆ‘ä»‹ç»
    print(result.get("chat_history"))
    """
    [HumanMessage(content='è¯·ä½ åšä¸€ä¸ªç®€å•çš„è‡ªæˆ‘ä»‹ç»', additional_kwargs={}, response_metadata={}), 
    AIMessage(content='ä½ å¥½ï¼æˆ‘æ˜¯Qwenï¼Œæ˜¯é˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹...', additional_kwargs={}, response_metadata={})]
    """
    ```



#### æ¨¡å‹æ¶ˆæ¯

LangChainæœ‰ä¸€äº›å†…ç½®çš„æ¶ˆæ¯ç±»å‹ï¼š

1. **SystemMessage** ï¼šè®¾å®šAIè¡Œä¸ºè§„åˆ™æˆ–èƒŒæ™¯ä¿¡æ¯ã€‚æ¯”å¦‚è®¾å®šAIçš„åˆå§‹çŠ¶æ€ã€è¡Œä¸ºæ¨¡å¼æˆ–å¯¹è¯çš„æ€»ä½“ç›®æ ‡ã€‚æ¯”å¦‚â€œä½œä¸ºä¸€ä¸ªä»£ç ä¸“å®¶â€ï¼Œæˆ–è€…â€œè¿”å›jsonæ ¼å¼â€ã€‚é€šå¸¸ä½œä¸ºè¾“å…¥æ¶ˆæ¯åºåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªä¼ é€’
2. **HumanMessage** ï¼šè¡¨ç¤ºæ¥è‡ªç”¨æˆ·è¾“å…¥ã€‚æ¯”å¦‚â€œå®ç° ä¸€ä¸ªå¿«é€Ÿæ’åºæ–¹æ³•â€
3. **AIMessage** ï¼šå­˜å‚¨AIå›å¤çš„å†…å®¹ã€‚è¿™å¯ä»¥æ˜¯æ–‡æœ¬ï¼Œä¹Ÿå¯ä»¥æ˜¯è°ƒç”¨å·¥å…·çš„è¯·æ±‚
4. **ChatMessage** ï¼šå¯ä»¥è‡ªå®šä¹‰è§’è‰²çš„é€šç”¨æ¶ˆæ¯ç±»å‹
5. **FunctionMessage/ToolMessage** ï¼šå‡½æ•°è°ƒç”¨/å·¥å…·æ¶ˆæ¯ï¼Œç”¨äºå‡½æ•°è°ƒç”¨ç»“æœçš„æ¶ˆæ¯ç±»å‹



#### ä¸Šä¸‹æ–‡è®°å¿†

åœ¨ä¸Šé¢æœ€åä¸€ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬æ˜¾ç¤ºç”³æ˜äº†è¦ä½¿ç”¨è®°å¿†èƒ½åŠ›ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç¨å¾®ä¿®æ”¹ä¸€ä¸‹è¾“å…¥ï¼Œè¿›è¡Œå¤šè½®å¯¹è¯ï¼Œè§‚å¯Ÿå›ç­”æ˜¯å¦æ­£ç¡®ï¼š

```python
import os
import dotenv

from langchain.chains.llm import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder, \
    HumanMessagePromptTemplate

from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# å®¢æˆ·ç«¯
client = ChatOpenAI(api_key=os.getenv("QW_API_KEY"),
                    base_url=os.getenv("QW_BASE_URL"),
                    model=os.getenv("QW_MODEL"),
                    max_tokens=48)

# prompt
prompt = ChatPromptTemplate(
    messages=[
        SystemMessagePromptTemplate.from_template("ä½ æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½AIå°åŠ©æ‰‹ï¼Œä½ çš„åå­—æ˜¯å°æ°"),
        MessagesPlaceholder(variable_name="chat_history"),  # ä»è®°å¿†å·¥å…·ä¸­ï¼Œé€šè¿‡ chat_history è¿™ä¸ªkeyè·å–æç¤ºä¸Šä¸‹æ–‡
        HumanMessagePromptTemplate.from_template("{question}",)  # å†™å…¥å ä½å˜é‡ï¼Œåç»­è°ƒç”¨ç»™è¿™ä¸ªkeyèµ‹å€¼
    ]
)

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
session = LLMChain(llm=client, prompt=prompt, memory=memory, verbose=True)

# å¤šè½®å¯¹è¯
result1 = session.invoke({"question": "ä¸­å›½çš„é™†åœ°é¢ç§¯æœ‰å¤šå°‘"})
print(result1.get("text"))  # ä½ å¥½ï¼Œæˆ‘æ˜¯å°æ°ï¼ä¸­å›½çš„é™†åœ°é¢ç§¯å¤§çº¦æ˜¯**960ä¸‡å¹³æ–¹å…¬é‡Œ**ï¼Œä½å±…ä¸–ç•Œç¬¬ä¸‰...

result2 = session.invoke({"question": "ä½ å«ä»€ä¹ˆåå­—å‘€ï¼Œæ˜¯ç”·ç”Ÿè¿˜æ˜¯å¥³ç”Ÿ"})
print(result2.get("text"))  # æˆ‘æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œåå­—å«â€œå°æ°â€ï¼Œå¬èµ·æ¥æœ‰ç‚¹åƒç”·ç”Ÿçš„åå­—ï¼Œä½†æˆ‘å…¶å®æ²¡æœ‰æ€§åˆ«å“¦ï½

"""
> Entering new LLMChain chain...
Prompt after formatting:
System: ä½ æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½AIå°åŠ©æ‰‹ï¼Œä½ çš„åå­—æ˜¯å°æ°
Human: ä¸­å›½çš„é™†åœ°é¢ç§¯æœ‰å¤šå°‘
AI: ä½ å¥½ï¼Œæˆ‘æ˜¯å°æ°ï¼ä¸­å›½çš„é™†åœ°é¢ç§¯å¤§çº¦æ˜¯**960ä¸‡å¹³æ–¹å…¬é‡Œ**ï¼Œä½å±…ä¸–ç•Œç¬¬ä¸‰...
Human: ä½ å«ä»€ä¹ˆåå­—å‘€ï¼Œæ˜¯ç”·ç”Ÿè¿˜æ˜¯å¥³ç”Ÿ

> Finished chain.
å—¨ï¼Œæˆ‘æ˜¯å°æ°ï¼ğŸ˜Š  
æˆ‘æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œåå­—å«â€œå°æ°â€ï¼Œå¬èµ·æ¥æœ‰ç‚¹åƒç”·ç”Ÿçš„åå­—ï¼Œä½†æˆ‘å…¶å®æ²¡æœ‰æ€§åˆ«å“¦ï½
"""
```

ä»è¾“å‡ºæ¥çœ‹ï¼Œå¯¹äºè®¾å®šçš„è§’è‰²ï¼Œç¡®å®æ˜¯æœ‰è®°å¿†æ•ˆæœçš„ã€‚ä½†å…¶å®å³ä¾¿æˆ‘ä»¬ä¸æ˜¾å¼ç”³æ˜ï¼ŒLangChainä¹Ÿæ˜¯ä¼šå¸®æˆ‘ä»¬è®°å¿†çš„ï¼š

```python
import os
import dotenv

from langchain_core.messages import SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# å®¢æˆ·ç«¯
client = ChatOpenAI(api_key=os.getenv("QW_API_KEY"),
                    base_url=os.getenv("QW_BASE_URL"),
                    model=os.getenv("QW_MODEL"),
                    max_tokens=48)

sys_message = SystemMessage(content="æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½çš„åŠ©æ‰‹ï¼Œæˆ‘çš„åå­—å«å°æ™º")
human_message = HumanMessage(content="çŒ«ç‹æ˜¯ä¸€åªçŒ«å—ï¼Ÿ")
human_message1 = HumanMessage(content="ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ")

messages = [sys_message, human_message, human_message1]

# è°ƒç”¨å¤§æ¨¡å‹ï¼Œä¼ å…¥messages
response = client.invoke(messages)
print(response.content)
"""
å“ˆå“ˆï¼Œè¿™ä¸ªé—®é¢˜å¾ˆæœ‰è¶£å‘¢ï¼çŒ«ç‹å…¶å®ä¸æ˜¯ä¸€åªçŒ«å“¦ï¼Œä»–æ˜¯ç¾å›½è‘—åçš„æ‘‡æ»šæ­Œæ‰‹åŸƒå°”ç»´æ–¯Â·æ™®é›·æ–¯åˆ©ï¼ˆElvis Presleyï¼‰çš„æ˜µç§°...

è‡³äºæˆ‘ï¼Œæˆ‘çš„åå­—å«å°æ™ºï¼Œæ˜¯ä½ çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œéšæ—¶å‡†å¤‡ä¸ºä½ æä¾›å¸®åŠ©å’Œè§£ç­”é—®é¢˜ï½æœ‰ä»€ä¹ˆæƒ³äº†è§£çš„å—ï¼ŸğŸ˜Š
"""

# å†å•ç‹¬é—®ä¸€ä¸‹
response = client.invoke([HumanMessage(content="æˆ‘åˆšæ‰æ˜¯ä¸æ˜¯é—®äº†ä¸€ä¸ªå…³äºçŒ«ç‹çš„é—®é¢˜ï¼Ÿ")])
print(response.content)
"""
ä½ åˆšæ‰æ²¡æœ‰é—®å…³äºçŒ«ç‹çš„é—®é¢˜å“¦ã€‚å¦‚æœä½ ç°åœ¨æƒ³äº†è§£çŒ«ç‹ï¼ˆElvis Presleyï¼‰çš„ä»»ä½•ä¿¡æ¯ï¼Œæ¯”å¦‚ä»–çš„éŸ³ä¹ã€ç”Ÿå¹³ã€å½±å“ç­‰ï¼Œæˆ‘å¾ˆä¹æ„ä¸ºä½ è§£ç­”ï¼ğŸ˜Š
"""
```

å¯ä»¥çœ‹åˆ°åœ¨åŒä¸€ä¸ªä¼šè¯å†…æ˜¯æœ‰è®°å¿†æ•ˆæœçš„ï¼Œæ–°çš„ä¼šè¯å°±å¤±å»äº†è®°å¿†æ•ˆæœã€‚è€Œåƒå‰é¢é‚£æ ·åˆ›å»ºä¸€ä¸ªä¼šè¯ï¼Œå°±å¯ä»¥æœ‰æ•ˆçš„ä¿ç•™å¯¹è¯çš„å†å²ä¿¡æ¯ã€‚



#### æ¨¡å‹è°ƒç”¨çš„æ–¹æ³•

LangChainä¸ºäº†å°½å¯èƒ½çš„ç®€åŒ–è‡ªå®šä¹‰é“¾çš„åˆ›å»ºï¼Œçº¦å®šäº†ä¸€ä¸ªRunnableåè®®ï¼ŒLangChainçš„è®¸å¤šç»„ä»¶ä¹Ÿéƒ½å®ç°äº†è¿™ä¸ªåè®®ï¼ŒåŒ…æ‹¬èŠå¤©æ¨¡å‹ã€æç¤ºè¯æ¨¡æ¿ã€è¾“å‡ºè§£æå™¨ã€æ£€ç´¢å™¨ã€ä»£ç†(æ™ºèƒ½ä½“)ç­‰ã€‚

**Runnable å®šä¹‰çš„å…¬å…±çš„è°ƒç”¨æ–¹æ³•å¦‚ä¸‹ï¼š**

- **invoke** : å¤„ç†å•æ¡è¾“å…¥ï¼Œç­‰å¾…LLMå®Œå…¨æ¨ç†å®Œæˆåå†è¿”å›è°ƒç”¨ç»“æœ

- **stream** : æµå¼å“åº”ï¼Œé€å­—è¾“å‡ºLLMçš„å“åº”ç»“æœ

- **batch** : å¤„ç†æ‰¹é‡è¾“å…¥

è¿™äº›ä¹Ÿæœ‰ç›¸åº”çš„å¼‚æ­¥æ–¹æ³•ï¼Œåº”è¯¥ä¸ asyncio çš„ **await** è¯­æ³•ä¸€èµ·ä½¿ç”¨ä»¥å®ç°å¹¶å‘ï¼š

- **astream** : å¼‚æ­¥æµå¼å“åº”
- **ainvoke** : å¼‚æ­¥å¤„ç†å•æ¡è¾“å…¥
- **abatch** : å¼‚æ­¥å¤„ç†æ‰¹é‡è¾“å…¥
- **astream_log** : å¼‚æ­¥æµå¼è¿”å›ä¸­é—´æ­¥éª¤ï¼Œä»¥åŠæœ€ç»ˆå“åº”
- **astream_events** : ï¼ˆæµ‹è¯•ç‰ˆï¼‰å¼‚æ­¥æµå¼è¿”å›é“¾ä¸­å‘ç”Ÿçš„äº‹ä»¶ï¼ˆåœ¨ langchain-core 0.1.14 ä¸­å¼•å…¥ï¼‰

å‰é¢æ‰€æœ‰çš„ç¤ºä¾‹ï¼Œå…¶å®éƒ½æ˜¯éæµå¼è¾“å‡ºçš„æ¼”ç¤ºã€‚è€Œæµå¼è¾“å‡ºæ˜¯ä¸€ç§æ›´å…·äº¤äº’æ„Ÿçš„è¾“å‡ºæ–¹å¼ï¼Œç”¨æˆ·ä¸å†éœ€è¦ç­‰å¾…å®Œæ•´ç­”æ¡ˆï¼Œè€Œæ˜¯èƒ½çœ‹åˆ°æ¨¡å‹**é€ä¸ª token** åœ°å®æ—¶è¿”å›å†…å®¹ã€‚é€‚åˆæ„å»ºå¼ºè°ƒâ€œå®æ—¶åé¦ˆâ€çš„åº”ç”¨ï¼Œå¦‚èŠå¤©æœºå™¨äººã€å†™ä½œåŠ©æ‰‹ç­‰ã€‚

Langchain ä¸­é€šè¿‡è®¾ç½® **streaming=True** å¹¶é…åˆ **å›è°ƒæœºåˆ¶** æ¥å¯ç”¨æµå¼è¾“å‡ºã€‚

```python
import os
import dotenv

from langchain_core.messages import SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# å®¢æˆ·ç«¯
client = ChatOpenAI(api_key=os.getenv("QW_API_KEY"),
                    base_url=os.getenv("QW_BASE_URL"),
                    model=os.getenv("QW_MODEL"),
                    max_tokens=48,
                    streaming=True)  # å¼€å¯æµå¼è¾“å‡º

human_message = HumanMessage(content="è¯·ä½ åšä¸€ä¸‹è‡ªæˆ‘ä»‹ç»")

# æµå¼è°ƒç”¨å¹¶è·å–å“åº”
resp = client.stream([human_message])

for chunk in resp:
    # end="" è¡¨ç¤ºæ‰“å°ä¸æ¢è¡Œ
    # flush=True åˆ·æ–°ç¼“å†²åŒºï¼Œåªè¦æœ‰å†…å®¹å°±ç«‹å³æ‰“å°
    print(chunk.content, end="", flush=True)

# è¾“å‡ºå†…å®¹å¦‚ä¸‹ï¼Œå°†ä»¥æ‰“å­—æœºæ•ˆæœçš„å½¢å¼è¾“å‡ºç»“æœ
"""
ä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼ˆQwenï¼‰ï¼Œæ˜¯é˜¿é‡Œäº‘ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œæ¯”å¦‚å†™æ•…äº‹ã€å†™å…¬æ–‡ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ç­‰ç­‰ï¼Œè¿˜èƒ½è¿›è¡Œé€»è¾‘æ¨ç†ã€ç¼–ç¨‹ï¼Œç”šè‡³è¡¨è¾¾è§‚ç‚¹ã€‚æˆ‘çš„ç›®æ ‡æ˜¯æˆä¸ºä½ å·¥ä½œå’Œç”Ÿæ´»ä¸­çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå¸®åŠ©ä½ æ›´é«˜æ•ˆåœ°å®Œæˆå„ç§ä»»åŠ¡ã€‚

å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼ğŸ˜Š
"""
```

å¦‚æœæˆ‘ä»¬æœ‰å¤šç»„ç›¸äº’éš”ç¦»çš„å¯¹è¯ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨æ‰¹é‡è°ƒç”¨ï¼š

```python
import os
import dotenv

from langchain_core.messages import SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# å®¢æˆ·ç«¯
client = ChatOpenAI(api_key=os.getenv("QW_API_KEY"),
                    base_url=os.getenv("QW_BASE_URL"),
                    model=os.getenv("QW_MODEL"),
                    max_tokens=48,
                    streaming=True)  # å¼€å¯æµå¼è¾“å‡º

msg1 = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä½æ•°å­¦è€å¸ˆ"),
    HumanMessage(content="è¯·åˆ†æä¸€å…ƒä¸€æ¬¡æ–¹ç¨‹çš„è§£é¢˜æ–¹æ³•")
]
msg2 = [
    SystemMessage(content="æ‚¨æ˜¯ä¸€ä½è¯—äºº"),
    HumanMessage(content="è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„äº”è¨€è¯—")
]

# è¿”å›å¤šä¸ªç»“æœ
resp = client.batch([msg1, msg2])

for res in resp:
    print(res.content)
```

å½“æœåŠ¡è¦æ‰¿æ‹…æ›´å¤šæµé‡æ—¶ï¼Œå¼‚æ­¥è¯·æ±‚å°†ä¼šå±•ç°å‡ºå¾ˆå¤§çš„ä¼˜åŠ¿ï¼Œä¸‹é¢å…ˆè¯•ä¸€ä¸‹æ¨¡å‹çš„å¼‚æ­¥è°ƒç”¨ï¼š

```python
import os
import dotenv
import asyncio

from langchain_core.messages import SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# å®¢æˆ·ç«¯
client = ChatOpenAI(api_key=os.getenv("QW_API_KEY"),
                    base_url=os.getenv("QW_BASE_URL"),
                    model=os.getenv("QW_MODEL"),
                    max_tokens=48,
                    streaming=True)  # å¼€å¯æµå¼è¾“å‡º


async def async_call():
    msg = [
        SystemMessage(content="æ‚¨æ˜¯ä¸€ä½è¯—äºº"),
        HumanMessage(content="è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„äº”è¨€è¯—")
    ]
    return await client.ainvoke(msg)


async def async_concurrent():
    tasks = [async_call() for _ in range(3)]
    return await asyncio.gather(*tasks)


if __name__ == '__main__':
    # è¿è¡Œå•ä¸ªå¼‚æ­¥ä»»åŠ¡
    result = asyncio.run(async_call())
    print(result.content)
    """
    ã€Šæ˜¥æœ›ã€‹
    æŸ³ç»¿æ˜ æºªæµï¼Œæ¡ƒçº¢ç…§çœ¼æ˜ã€‚
    èºå•¼èŠ±è‡ªè½ï¼Œé£æš–è‰åˆç”Ÿã€‚
    äº‘æ·¡å¤©å…‰è¿‘ï¼Œå±±é’é‡å¾„å¹³ã€‚
    æ˜¥æ·±äººæœªè§‰ï¼Œå¤„å¤„æœ‰æ–°å£°ã€‚
    """

    # å¹¶å‘è¿è¡Œå¤šä¸ªå¼‚æ­¥ä»»åŠ¡
    results = asyncio.run(async_concurrent())
    for res in results:
        print(res.content)  # ä¸‰ä¸ªç»“æœæ­£å¸¸è¾“å‡º
```



### Prompt Template

Prompt Templateæ˜¯LangChainä¸­çš„ä¸€ä¸ªæ¦‚å¿µï¼Œæ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼Œè¿”å›ä¸€ä¸ªä¼ é€’ç»™LLMçš„ä¿¡æ¯ï¼ˆå³æç¤ºè¯ï¼‰ã€‚åœ¨åº”ç”¨å¼€å‘ä¸­ï¼Œå›ºå®šçš„æç¤ºè¯é™åˆ¶äº†æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚ç”¨èŒƒå›´ã€‚æ‰€ä»¥ï¼Œprompt template æ˜¯ä¸€ä¸ª**æ¨¡æ¿åŒ–çš„å­—ç¬¦ä¸²ï¼ˆå ä½ç¬¦ï¼‰**ï¼Œä½ å¯ä»¥å°†**å˜é‡æ’å…¥åˆ°æ¨¡æ¿**ä¸­ï¼Œä»è€Œåˆ›å»ºå‡ºä¸åŒçš„æç¤ºã€‚è°ƒç”¨æ—¶ï¼š

- ä»¥**å­—å…¸**ä½œä¸ºè¾“å…¥ï¼Œå…¶ä¸­æ¯ä¸ªé”®ä»£è¡¨è¦å¡«å……çš„æç¤ºæ¨¡æ¿ä¸­çš„å˜é‡ã€‚

- è¾“å‡ºä¸€ä¸ª**PromptValue**ã€‚è¿™ä¸ª PromptValue å¯ä»¥ä¼ é€’ç»™ LLM æˆ– ChatModelï¼Œå¹¶ä¸”è¿˜å¯ä»¥è½¬æ¢ 

  ä¸ºå­—ç¬¦ä¸²æˆ–æ¶ˆæ¯åˆ—è¡¨ã€‚



**æ¨¡æ¿ç±»å‹ï¼š**

- **PromptTemplate**ï¼šLLMæç¤ºæ¨¡æ¿ï¼Œç”¨äºç”Ÿæˆå­—ç¬¦ä¸²æç¤ºã€‚å®ƒä½¿ç”¨ Python çš„å­—ç¬¦ä¸²æ¥æ¨¡æ¿æç¤ºã€‚ 
- **ChatPromptTemplate**ï¼šèŠå¤©æç¤ºæ¨¡æ¿ï¼Œç”¨äºç»„åˆå„ç§è§’è‰²çš„æ¶ˆæ¯æ¨¡æ¿ï¼Œä¼ å…¥èŠå¤©æ¨¡å‹ã€‚ 
- **XxxMessagePromptTemplate**ï¼šæ¶ˆæ¯æç¤ºè¯æ¨¡æ¿ï¼ŒåŒ…æ‹¬ï¼šSystemMessagePromptTemplateã€ HumanMessagePromptTemplateã€AIMessagePromptTemplateã€ ChatMessagePromptTemplateç­‰ 
- **FewShotPromptTemplate**ï¼šæ ·æœ¬æç¤ºè¯æ¨¡æ¿ï¼Œé€šè¿‡ç¤ºä¾‹æ¥æ•™æ¨¡å‹å¦‚ä½•å›ç­”
- **PipelinePrompt**ï¼šç®¡é“æç¤ºè¯æ¨¡æ¿ï¼Œç”¨äºæŠŠå‡ ä¸ªæç¤ºè¯ç»„åˆåœ¨ä¸€èµ·ä½¿ç”¨ã€‚ 
- è‡ªå®šä¹‰æ¨¡æ¿ ï¼šå…è®¸åŸºäºå…¶å®ƒæ¨¡æ¿ç±»æ¥å®šåˆ¶è‡ªå·±çš„æç¤ºè¯æ¨¡æ¿ã€‚



#### PromptTemplate

PromptTemplateç±»ï¼Œç”¨äºå¿«é€Ÿæ„å»º åŒ…å«å˜é‡ çš„æç¤ºè¯æ¨¡æ¿ï¼Œå¹¶é€šè¿‡ ä¼ å…¥ä¸åŒçš„å‚æ•°å€¼ ç”Ÿæˆè‡ªå®šä¹‰çš„æç¤ºè¯ã€‚æ•´ä½“æ„Ÿå—ä¸‹æ¥å’ŒPythonåŸç”Ÿçš„æ ¼å¼åŒ–å·®ä¸å¤šã€‚

```python
from langchain_core.prompts import PromptTemplate

# æ¨¡æ¿åˆ›å»ºæ–¹å¼ä¸€ï¼šå®ä¾‹åŒ–åˆ›å»ºæ¨¡æ¿
template1 = PromptTemplate(
    template="è¯·è¿”å›Androidç«¯åº”ç”¨çš„åŒ…åï¼Œåº”ç”¨åç§°æ˜¯: {app}ï¼Œå¹¶ç®€è¦ä»‹ç»åº”ç”¨çš„åŠŸèƒ½ï¼Œæ§åˆ¶åœ¨{num}å­—ä»¥å†…",
    input_values=["app", "num"])

# æ¨¡æ¿åˆ›å»ºæ–¹å¼äºŒï¼šä½¿ç”¨from_templateæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä¸é…ç½®å˜é‡
template2 = PromptTemplate.from_template(
    "è¯·è¿”å›Androidç«¯åº”ç”¨çš„åŒ…åï¼Œåº”ç”¨åç§°æ˜¯: {app}ï¼Œå¹¶ç®€è¦ä»‹ç»åº”ç”¨çš„åŠŸèƒ½ï¼Œæ§åˆ¶åœ¨{num}å­—ä»¥å†…")

# ä½¿ç”¨æ¨¡æ¿ï¼Œä¹Ÿå°±æ˜¯å®Œæˆæ ¼å¼åŒ–çš„è¿‡ç¨‹
template1 = template1.format(app="å¾®ä¿¡", num=20)
template2 = template2.format(app="QQ", num=30)

print(template1)
print(template2)
"""
è¯·è¿”å›Androidç«¯åº”ç”¨çš„åŒ…åï¼Œåº”ç”¨åç§°æ˜¯: å¾®ä¿¡ï¼Œå¹¶ç®€è¦ä»‹ç»åº”ç”¨çš„åŠŸèƒ½ï¼Œæ§åˆ¶åœ¨20å­—ä»¥å†…
è¯·è¿”å›Androidç«¯åº”ç”¨çš„åŒ…åï¼Œåº”ç”¨åç§°æ˜¯: QQï¼Œå¹¶ç®€è¦ä»‹ç»åº”ç”¨çš„åŠŸèƒ½ï¼Œæ§åˆ¶åœ¨30å­—ä»¥å†…
"""
```

å¦å¤–ï¼Œå› ä¸ºLangChainä¸­æ¨¡æ¿ç›¸å…³çš„ç±»ä¹Ÿæ˜¯å®ç°äº†Runnaleåè®®çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨invokeæ–¹æ³•ï¼Œæ¥å®Œæˆæ ¼å¼åŒ–ã€‚å…¶ä¸­formatç›´æ¥è¿”å›å€¼ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼›invokeåˆ™è¿”å›å€¼ä¸ºPromptValueç±»å‹ï¼Œæ¥ç€è°ƒç”¨to_string()è¿”å›å­—ç¬¦ä¸²ã€‚çœ‹ä¸‹é¢çš„ä¾‹å­ï¼š

```python
import os
import dotenv

from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

template = PromptTemplate.from_template(
    "è¯·è¿”å›Androidç«¯åº”ç”¨çš„åŒ…åï¼Œåº”ç”¨åç§°æ˜¯: {app}ï¼Œå¹¶ç®€è¦ä»‹ç»åº”ç”¨çš„åŠŸèƒ½ï¼Œæ§åˆ¶åœ¨{num}å­—ä»¥å†…")

# ä½¿ç”¨invokeè¿”å›PromptValueå®ä¾‹ï¼Œå¯ç›´æ¥ç»™å®¢æˆ·ç«¯ä½¿ç”¨
# éœ€è¦æ³¨æ„çš„æ—¶ä½¿ç”¨invokeéœ€è¦ä¼ å…¥å­—å…¸
template = template.invoke({"app": "QQ", "num": 30})

client = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                    model=os.getenv("DS_MODEL"), max_tokens=30)

result = client.invoke(template)

print(result.content)
"""
QQçš„Androidç«¯åŒ…åä¸ºï¼š`com.tencent.mobileqq`  

åŠŸèƒ½ç®€ä»‹ï¼šå³æ—¶é€šè®¯ç¤¾äº¤åº”ç”¨ï¼Œæ”¯æŒæ–‡å­—ã€è¯­éŸ³ã€
"""
```



#### ChatPromptTemplate

ChatPromptTemplateæ˜¯åˆ›å»º**èŠå¤©æ¶ˆæ¯åˆ—è¡¨**æç¤ºæ¨¡æ¿ã€‚å®ƒæ¯”æ™®é€š PromptTemplate æ›´é€‚åˆå¤„ç†å¤šè§’è‰²ã€å¤šè½®æ¬¡çš„å¯¹è¯åœºæ™¯ã€‚

**ç‰¹ç‚¹ï¼š **

- æ”¯æŒ System / Human / AI ç­‰ä¸åŒè§’è‰²çš„æ¶ˆæ¯æ¨¡æ¿ 
- å¯¹è¯å†å²ç»´æŠ¤ 

**å‚æ•°ç±»å‹ï¼š** åˆ—è¡¨å‚æ•°æ ¼å¼æ˜¯tupleç±»å‹ï¼ˆ role :str content :str ç»„åˆæœ€å¸¸ç”¨ï¼‰ 

- å…ƒç»„çš„æ ¼å¼ä¸ºï¼š (role: str | type, content: str | list[dict] | list[object]) 
- å…¶ä¸­ role æ˜¯ï¼šå­—ç¬¦ä¸²ï¼ˆå¦‚ "system" ã€ "human" ã€ "ai" ï¼‰

```PYTHON
import os
import dotenv

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# ç›´æ¥åŸºäºæ¶ˆæ¯åˆ—è¡¨æ„å»ºæ¨¡æ¿
template = ChatPromptTemplate.from_messages(
    [
        ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIæœºå™¨äººï¼Œä½ çš„åå­—æ˜¯{name}ã€‚"),
        ("human", "ä½ å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ"),
        ("ai", "æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢ï¼"),
        ("human", "{query}")
    ]
)

template = template.invoke({"name": "é˜¿ç»´å¡”", "query": "ä¸­å›½æœ‰å¤šå°‘ä¸ªçœä»½ï¼Œè¿”å›ç»Ÿè®¡æ•°æ®å³å¯"})

client = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                    model=os.getenv("DS_MODEL"), max_tokens=30)

result = client.invoke(template)

print(result.content)
"""
ä¸­å›½å…±æœ‰23ä¸ªçœã€5ä¸ªè‡ªæ²»åŒºã€4ä¸ªç›´è¾–å¸‚å’Œ2ä¸ªç‰¹åˆ«è¡Œæ”¿åŒºï¼Œæ€»è®¡34ä¸ªçœçº§è¡Œæ”¿åŒºã€‚
"""
```

ä¸Šé¢æ¼”ç¤ºäº†åˆ—è¡¨å…ƒç´ æ—¶å…ƒç»„ç±»å‹çš„æ¨¡æ¿æ„å»ºï¼Œä½†å®é™…ä¸Šåˆ—è¡¨çš„å…ƒç´ å¯ä»¥æ˜¯å­—ç¬¦ä¸²ã€å­—å…¸ã€å­—ç¬¦ä¸²æ„æˆçš„å…ƒç»„ã€æ¶ˆæ¯ç±»å‹ã€æç¤ºè¯æ¨¡æ¿ç±»å‹ã€æ¶ˆæ¯æç¤ºè¯æ¨¡æ¿ç±»å‹ç­‰ã€‚å½“åˆ—è¡¨å…ƒç´ æ˜¯å­—ç¬¦ä¸²æ—¶ï¼Œæ¨¡æ¿ç±»ä¼šé»˜è®¤æ¯ä¸ªå†…å®¹çš„roleéƒ½æ˜¯humanï¼Œå› æ­¤éå¸¸ä¸å»ºè®®ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²åˆ—è¡¨ã€‚

ä¸‹é¢æ¼”ç¤ºä¸€ä¸‹å­—å…¸æ„å»ºæ¨¡æ¿å’Œæ¶ˆæ¯ç±»å‹æ„å»ºæ¨¡æ¿ï¼š

```PYTHON
import os
import dotenv
from langchain_core.messages import SystemMessage, HumanMessage

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# åŸºäºå­—å…¸ï¼Œéœ€è¦ç”³æ˜è§’è‰²å’Œå†…å®¹çš„key
template1 = ChatPromptTemplate.from_messages(
    [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIæœºå™¨äººï¼Œä½ çš„åå­—æ˜¯{name}ã€‚"},
        {"role": "human", "content": "ä½ å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ"}
    ]
)

# åŸºäºæ¶ˆæ¯ç±»ï¼Œå¯ä»¥çœæ‰roleçš„ç”³æ˜
# éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ­¤å¤„åˆ—è¡¨å…ƒç´ æ˜¯æ¶ˆæ¯ç±»å‹ï¼Œå¹¶ä¸æ˜¯æ¨¡æ¿ï¼Œæ‰€ä»¥æ˜¯ä¸èƒ½å®Œæˆå˜é‡æ›¿æ¢çš„
template2 = ChatPromptTemplate.from_messages(
    [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIæœºå™¨äººï¼Œä½ çš„åå­—æ˜¯{name}ã€‚"),
        HumanMessage(content="ä½ å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ")
    ]
)

template1 = template1.invoke({"name": "é˜¿ç»´å¡”"})
template2 = template2.invoke({"name": "å’•å™œ"})

print(template1)
print(template2)
"""
messages=[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIæœºå™¨äººï¼Œä½ çš„åå­—æ˜¯é˜¿ç»´å¡”ã€‚', ...
messages=[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIæœºå™¨äººï¼Œä½ çš„åå­—æ˜¯{name}ã€‚', ...
"""
```



#### MessagePromptTemplate

ä¸Šé¢ç¤ºä¾‹ä¸­å¯ä»¥çœ‹åˆ°æ¶ˆæ¯ç±»å‹æ˜¯ä¸èƒ½è®¾å¤‡å˜é‡å®Œæˆæ›¿æ¢çš„ã€‚LangChainæä¾›ä¸åŒç±»å‹çš„MessagePromptTemplateã€‚æœ€å¸¸ç”¨çš„æ˜¯ SystemMessagePromptTemplate ã€HumanMessagePromptTemplate å’Œ AIMessagePromptTemplate ï¼Œåˆ†åˆ«åˆ›å»ºç³»ç»Ÿæ¶ˆæ¯ã€äººå·¥æ¶ˆæ¯å’ŒAIæ¶ˆæ¯ï¼Œå®ƒä»¬æ˜¯ChatMessagePromptTemplateçš„ç‰¹å®šè§’è‰²å­ç±»ï¼Œå°±å¯ä»¥è½»æ¾å®ç°å˜é‡çš„æ›¿æ¢äº†ã€‚çœ‹ä¸‹é¢çš„ç¤ºä¾‹ï¼š

```python
import os
import dotenv
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate, \
    ChatMessagePromptTemplate

from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

template = ChatPromptTemplate.from_messages(
    [
        # ä½¿ç”¨æ¶ˆæ¯ç±»å‹
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIæœºå™¨äººï¼Œä½ çš„åå­—æ˜¯å’•å™œã€‚"),
        HumanMessage(content="ä½ å¥½ï¼Œè¯·é—®ä½ æ˜¯å’•å™œå—ï¼Ÿ"),

        # ä½¿ç”¨æ¶ˆæ¯æ¨¡æ¿ç±»å‹
        AIMessagePromptTemplate.from_template("æ˜¯çš„ï¼Œæˆ‘æ˜¯å’•å™œæ™ºèƒ½æœºå™¨äººï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨å—ï¼Ÿ"),
        HumanMessagePromptTemplate.from_template("{input}"),

        # æ¶ˆæ¯æ¨¡æ¿çš„åŸºç±» ChatMessagePromptTemplate å¯ä»¥å®ç°è‡ªå®šä¹‰çš„è§’è‰²
        ChatMessagePromptTemplate.from_template(template="{answer}", role="ai"),

        # å¦å¤– ChatPromptTemplate æœ¬èº«ä¹Ÿæ˜¯å¯ä»¥åµŒå¥—çš„
        ChatPromptTemplate.from_messages([("human", "{query}")])
    ]
)

template = template.invoke({"input": "ä¸­å›½ä¸€å…±æœ‰å¤šå°‘ä¸ªçœçº§è¡Œæ”¿åŒº",
                            "answer": "ä¸­å›½ä¸€å…±æœ‰34ä¸ªå‡çº§è¡Œæ”¿åŒºï¼ŒåŒ…å«çœã€ç›´è¾–å¸‚ã€è‡ªæ²»åŒºã€ç‰¹åˆ«è¡Œæ”¿åŒº",
                            "query": "é‚£ä¸­å›½æœ‰å¤šå°‘ä¸ªåŸå¸‚å‘¢ï¼Œä¸åŒ…æ‹¬åœ°çº§å¸‚"})
print(template)
"""
messages=[
SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIæœºå™¨äººï¼Œä½ çš„åå­—æ˜¯å’•å™œã€‚', additional_kwargs={}, response_metadata={}), 
HumanMessage(content='ä½ å¥½ï¼Œè¯·é—®ä½ æ˜¯å’•å™œå—ï¼Ÿ', additional_kwargs={}, response_metadata={}), 
AIMessage(content='æ˜¯çš„ï¼Œæˆ‘æ˜¯å’•å™œæ™ºèƒ½æœºå™¨äººï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨å—ï¼Ÿ', additional_kwargs={}, response_metadata={}), 
HumanMessage(content='ä¸­å›½ä¸€å…±æœ‰å¤šå°‘ä¸ªçœçº§è¡Œæ”¿åŒº', additional_kwargs={}, response_metadata={}), 
ChatMessage(content='ä¸­å›½ä¸€å…±æœ‰34ä¸ªå‡çº§è¡Œæ”¿åŒºï¼ŒåŒ…å«çœã€ç›´è¾–å¸‚ã€è‡ªæ²»åŒºã€ç‰¹åˆ«è¡Œæ”¿åŒº', additional_kwargs={}, response_metadata={}, role='ai'),
HumanMessage(content='é‚£ä¸­å›½æœ‰å¤šå°‘ä¸ªåŸå¸‚å‘¢ï¼Œä¸åŒ…æ‹¬åœ°çº§å¸‚', additional_kwargs={}, response_metadata={})]
"""
```

å¯ä»¥çœ‹åˆ°ï¼Œæ¨¡æ¿æ¶ˆæ¯ç±»å‹åœ¨æ ¼å¼åŒ–ä¹‹åä¹Ÿéƒ½è½¬æˆäº†å¯¹åº”çš„æ¶ˆæ¯ç±»å‹ã€‚

å½“ä½ ä¸ç¡®å®šæ¶ˆæ¯æç¤ºæ¨¡æ¿ä½¿ç”¨ä»€ä¹ˆè§’è‰²ï¼Œæˆ–è€…å¸Œæœ›åœ¨æ ¼å¼åŒ–è¿‡ç¨‹ä¸­ æ’å…¥æ¶ˆæ¯åˆ—è¡¨ æ—¶ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿ è¿™å°±éœ€è¦ä½¿ç”¨ MessagesPlaceholderï¼Œè´Ÿè´£åœ¨ç‰¹å®šä½ç½®æ·»åŠ æ¶ˆæ¯åˆ—è¡¨ã€‚åœ¨å¤šè½®å¯¹è¯ç³»ç»Ÿå­˜å‚¨å†å²æ¶ˆæ¯ä»¥åŠAgentçš„ä¸­é—´æ­¥éª¤å¤„ç†æ­¤åŠŸèƒ½éå¸¸æœ‰ç”¨ã€‚

```PYTHON
import os
import dotenv
from langchain_core.messages import HumanMessage, AIMessage

from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, \
    SystemMessagePromptTemplate, MessagesPlaceholder

from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

template = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template("ä½ æ˜¯{role}"),

        # æ¶ˆæ¯æç¤º
        MessagesPlaceholder(variable_name="history"),

        HumanMessagePromptTemplate.from_template("{query}")
    ]
)

# æ„å»ºæ¶ˆæ¯å†å²
history = [HumanMessage(content="1+2*3=?"), AIMessage(content="1+2*3=7")]

# æ¨¡æ¿æ ¼å¼åŒ–
# è¿™é‡Œæ˜¯æ‰‹åŠ¨é…ç½®historyï¼Œåœ¨ä¸€ä¸ªä¼šè¯ä¸­å¯ä»¥ä½¿ç”¨memoryçš„å®ç°ç»˜ç”»å†å²çš„è‡ªåŠ¨å¡«å……ã€‚å‰é¢å·²æœ‰ä»£ç æ¼”ç¤ºè¿‡
template = template.invoke({"role": "æ™ºèƒ½è®¡ç®—å™¨",
                            "history": history,
                            "query": "7-2*3+5=?"})

client = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                    model=os.getenv("DS_MODEL"), max_tokens=30)

response = client.invoke(template)
print(response.content)  # 7 - 2 * 3 + 5 = 7 - 6 + 5 = 1 + 5 = 6
```



#### FewShotPromptTemplate

åœ¨æ„å»ºpromptæ—¶ï¼Œå¯ä»¥é€šè¿‡æ„å»ºä¸€ä¸ª**å°‘é‡ç¤ºä¾‹åˆ—è¡¨**å»è¿›ä¸€æ­¥æ ¼å¼åŒ–promptï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ä½†å¼ºå¤§çš„æŒ‡ å¯¼ç”Ÿæˆçš„æ–¹å¼ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥**æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½**ã€‚ 

å°‘é‡ç¤ºä¾‹æç¤ºæ¨¡æ¿å¯ä»¥ç”±**ä¸€ç»„ç¤ºä¾‹**æˆ–ä¸€ä¸ªè´Ÿè´£ä»å®šä¹‰çš„é›†åˆä¸­é€‰æ‹©**ä¸€éƒ¨åˆ†ç¤ºä¾‹**çš„ç¤ºä¾‹é€‰æ‹©å™¨æ„å»ºã€‚ 

- å‰è€…ï¼šä½¿ç”¨ FewShotPromptTemplate æˆ– FewShotChatMessagePromptTemplate 
- åè€…ï¼šä½¿ç”¨ Example selectors(ç¤ºä¾‹é€‰æ‹©å™¨) 

æ¯ä¸ªç¤ºä¾‹çš„ç»“æ„éƒ½æ˜¯ä¸€ä¸ª**å­—å…¸**ï¼Œå…¶ä¸­**é”®**æ˜¯è¾“å…¥å˜é‡ï¼Œ**å€¼**æ˜¯è¾“å…¥å˜é‡çš„å€¼ã€‚



##### FewShotPromptTemplate

ä¸‹é¢æ¼”ç¤ºFewShotPromptTemplateçš„ç”¨æ³•ï¼š

```python
import os
import dotenv
from langchain_core.messages import HumanMessage, AIMessage

from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# å…ˆåˆ›å»ºæç¤ºæ¨¡æ¿ï¼Œç”¨äºè®¾ç½®æ¯ä¸ªç¤ºä¾‹çš„æ ¼å¼
prompt_sample = PromptTemplate.from_template("ä½ æ˜¯ä¸€ä¸ªæ•°å­¦ä¸“å®¶ã€‚ç®—å¼:{input} å€¼:{output} ä½¿ç”¨:{description}")

# ç¤ºä¾‹
examples = [
    {"input": "2+2", "output": "4", "description": "åŠ æ³•è¿ç®—"},
    {"input": "5-2", "output": "3", "description": "å‡æ³•è¿ç®—"}
]

# åˆ›å»ºç¤ºä¾‹promptæ¨¡æ¿
prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=prompt_sample,
    suffix="ä½ æ˜¯ä¸€ä¸ªæ•°å­¦ä¸“å®¶ã€‚ç®—å¼:{input} å€¼:{output}",  # åœ¨ç¤ºä¾‹åé¢åŠ ä¸Šè¦å¤§æ¨¡å‹å¤„ç†çš„é—®é¢˜
    input_variables=["input", "output"]  # ç”³æ˜æ›¿æ¢å˜é‡
)

# æ¨¡æ¿æ ¼å¼åŒ–
prompt = prompt.invoke({"input": "3*4", "output": "12"})
print(prompt)
"""
text='ä½ æ˜¯ä¸€ä¸ªæ•°å­¦ä¸“å®¶ã€‚ç®—å¼:2+2 å€¼:4 ä½¿ç”¨:åŠ æ³•è¿ç®—\n\nä½ æ˜¯ä¸€ä¸ªæ•°å­¦ä¸“å®¶ã€‚ç®—å¼:5-2 å€¼:3 ä½¿ç”¨:å‡æ³•è¿ç®—\n\nä½ æ˜¯ä¸€ä¸ªæ•°å­¦ä¸“å®¶ã€‚ç®—å¼:3*4 å€¼:12'
"""

client = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                    model=os.getenv("DS_MODEL"), max_tokens=30)

response = client.invoke(prompt)
print(response.content)  # ä½¿ç”¨:ä¹˜æ³•è¿ç®—
```



##### FewShotChatMessagePromptTemplate

é™¤äº†FewShotPromptTemplateä¹‹å¤–ï¼ŒFewShotChatMessagePromptTemplateæ˜¯ä¸“é—¨ä¸º**èŠå¤©å¯¹è¯åœºæ™¯**è®¾è®¡çš„å°‘æ ·æœ¬ï¼ˆfew-shotï¼‰æç¤ºæ¨¡æ¿ï¼Œå®ƒç»§æ‰¿è‡ª FewShotPromptTemplate ï¼Œä½†é’ˆå¯¹èŠå¤©æ¶ˆæ¯çš„æ ¼å¼è¿›è¡Œäº†ä¼˜åŒ–ã€‚

**ç‰¹ç‚¹ï¼š** 

- è‡ªåŠ¨å°†ç¤ºä¾‹æ ¼å¼åŒ–ä¸ºèŠå¤©æ¶ˆæ¯ï¼ˆ HumanMessage / AIMessage ç­‰ï¼‰ 
- è¾“å‡ºç»“æ„åŒ–èŠå¤©æ¶ˆæ¯ï¼ˆ List[BaseMessage] ï¼‰ 
- ä¿ç•™å¯¹è¯è½®æ¬¡ç»“æ„

```PYTHON
import os
import dotenv
from langchain_core.messages import HumanMessage, AIMessage

from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate, ChatPromptTemplate, \
    FewShotChatMessagePromptTemplate

from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# å…ˆåˆ›å»ºç¤ºä¾‹æ¨¡æ¿ï¼Œç”¨äºè®¾ç½®æ¯ä¸ªç¤ºä¾‹çš„æ ¼å¼
prompt_sample = ChatPromptTemplate.from_messages([
    ("human", "{input} è®¡ç®—ç»“æœæ˜¯å¤šå°‘ï¼Ÿ"),
    ("ai", "{output}")
])

# ç¤ºä¾‹
examples = [
    {"input": "2ğŸ¦œ2", "output": "4"},
    {"input": "5ğŸ¦œ2", "output": "7"}
]

# ç¤ºä¾‹æ¶ˆæ¯
few_shot_msg_prompt = FewShotChatMessagePromptTemplate(
    examples=examples,
    example_prompt=prompt_sample
)

# å®Œæ•´æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæ•°å­¦ä¸“å®¶ã€‚"),
    few_shot_msg_prompt,
    ("human", "{input} è®¡ç®—ç»“æœæ˜¯å¤šå°‘ï¼Ÿ")
])

# æ¨¡æ¿æ ¼å¼åŒ–
prompt = prompt.invoke({"input": "7ğŸ¦œ6"})
print(prompt)
"""
messages=[
SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªæ•°å­¦ä¸“å®¶ã€‚', additional_kwargs={}, response_metadata={}), 
HumanMessage(content='2ğŸ¦œ2 è®¡ç®—ç»“æœæ˜¯å¤šå°‘ï¼Ÿ', additional_kwargs={}, response_metadata={}), 
AIMessage(content='4', additional_kwargs={}, response_metadata={}), 
HumanMessage(content='5ğŸ¦œ2 è®¡ç®—ç»“æœæ˜¯å¤šå°‘ï¼Ÿ', additional_kwargs={}, response_metadata={}), 
AIMessage(content='7', additional_kwargs={}, response_metadata={}), 
HumanMessage(content='7ğŸ¦œ6 è®¡ç®—ç»“æœæ˜¯å¤šå°‘ï¼Ÿ', additional_kwargs={}, response_metadata={})]
"""

client = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                    model=os.getenv("DS_MODEL"), max_tokens=30)

response = client.invoke(prompt)
print(response.content)  # 13
```



##### ExampleSelectors

å‰é¢FewShotPromptTemplateçš„ç‰¹ç‚¹æ˜¯ï¼Œæ— è®ºè¾“å…¥ä»€ä¹ˆé—®é¢˜ï¼Œéƒ½ä¼šåŒ…å«å…¨éƒ¨ç¤ºä¾‹ã€‚åœ¨å®é™…å¼€å‘ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®å½“å‰è¾“å…¥ï¼Œä½¿ç”¨ç¤ºä¾‹é€‰æ‹©å™¨ï¼Œä»å¤§é‡å€™é€‰ç¤ºä¾‹ä¸­é€‰å–æœ€ç›¸å…³çš„ç¤ºä¾‹å­é›†ã€‚

**ä½¿ç”¨çš„å¥½å¤„ï¼š**é¿å…ç›²ç›®ä¼ é€’æ‰€æœ‰ç¤ºä¾‹ï¼Œå‡å°‘ token æ¶ˆè€—çš„åŒæ—¶ï¼Œè¿˜å¯ä»¥æå‡è¾“å‡ºæ•ˆæœ

**ç¤ºä¾‹é€‰æ‹©ç­–ç•¥ï¼š**è¯­ä¹‰ç›¸ä¼¼é€‰æ‹©ã€é•¿åº¦é€‰æ‹©ã€æœ€å¤§è¾¹é™…ç›¸å…³ç¤ºä¾‹é€‰æ‹©ç­‰ 

- è¯­ä¹‰ç›¸ä¼¼é€‰æ‹© ï¼šé€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦ç­‰åº¦é‡æ–¹å¼è¯„ä¼°è¯­ä¹‰ç›¸å…³æ€§ï¼Œé€‰æ‹©ä¸è¾“å…¥é—®é¢˜æœ€ç›¸ä¼¼çš„ k ä¸ªç¤ºä¾‹ã€‚ 
- é•¿åº¦é€‰æ‹© ï¼šæ ¹æ®è¾“å…¥æ–‡æœ¬çš„é•¿åº¦ï¼Œä»å€™é€‰ç¤ºä¾‹ä¸­ç­›é€‰å‡ºé•¿åº¦æœ€åŒ¹é…çš„ç¤ºä¾‹ã€‚å¢å¼ºæ¨¡å‹å¯¹æ–‡æœ¬ç»“æ„çš„ç†è§£ã€‚æ¯”è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—æ›´è½»é‡ï¼Œé€‚åˆå¯¹å“åº”é€Ÿåº¦è¦æ±‚é«˜çš„åœºæ™¯ã€‚ 
- æœ€å¤§è¾¹é™…ç›¸å…³ç¤ºä¾‹é€‰æ‹© ï¼šä¼˜å…ˆé€‰æ‹©ä¸è¾“å…¥é—®é¢˜è¯­ä¹‰ç›¸ä¼¼çš„ç¤ºä¾‹ï¼›åŒæ—¶ï¼Œé€šè¿‡æƒ©ç½šæœºåˆ¶é¿å…è¿”å›åŒè´¨åŒ–çš„å†…å®¹

FAISSå’ŒChromaæ˜¯ä¸¤ç§å¸¸ç”¨çš„å‘é‡æ•°æ®åº“ï¼Œæ ¸å¿ƒåŠŸèƒ½å‡ä¸ºå­˜å‚¨å’Œæ£€ç´¢å‘é‡æ•°æ®ã€‚Chromaæ›´é€‚åˆæŒä¹…åŒ–å­˜å‚¨å‘é‡æ•°æ®ï¼Œæ­¤å¤„æ¼”ç¤ºä»…åœ¨å†…å­˜ä¸­ä¿å­˜ï¼Œä½¿ç”¨faissä½œä¸ºæ¼”ç¤ºï¼Œå…ˆå®‰è£…ä¾èµ–ï¼š

```shell
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple/ faiss-cpu langchain_community
```

æ ·æœ¬é€‰æ‹©ç¤ºä¾‹å¦‚ä¸‹ï¼ŒåŸºäºå‘é‡è®¡ç®—å±äºè¯­ä¹‰ç›¸ä¼¼é€‰æ‹©ï¼š

```PYTHON
import os
import dotenv

from langchain_community.vectorstores import FAISS
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

dotenv.load_dotenv()

# 1. ç¤ºä¾‹æ•°æ®
examples = [
    {"input": "é«˜å…´", "output": "æ‚²ä¼¤"},
    {"input": "é«˜", "output": "çŸ®"},
    {"input": "é•¿", "output": "çŸ­"},
    {"input": "ç²¾åŠ›å……æ²›", "output": "æ— ç²¾æ‰“é‡‡"},
    {"input": "é˜³å…‰", "output": "é˜´æš—"},
    {"input": "ç²—ç³™", "output": "å…‰æ»‘"},
    {"input": "å¹²ç‡¥", "output": "æ½®æ¹¿"},
    {"input": "å¯Œè£•", "output": "è´«ç©·"},
]

# 2.å®šä¹‰åµŒå…¥æ¨¡å‹
embeddings = OpenAIEmbeddings(api_key=os.getenv("API_KEY"),
                              base_url=os.getenv("API_BASE"),
                              model="text-embedding-ada-002")

# 3. åˆ›å»ºè¯­ä¹‰ç›¸ä¼¼æ€§é€‰æ‹©å™¨ï¼Œkè¡¨ç¤ºè¿”å›æœ€ç›¸ä¼¼çš„ä¸¤ä¸ªç¤ºä¾‹
selector = SemanticSimilarityExampleSelector.from_examples(examples=examples, embeddings=embeddings,
                                                           vectorstore_cls=FAISS, k=2)

# 4. åˆ›å»ºç¤ºä¾‹æ ¼å¼æ¨¡æ¿
prompt_sample = ChatPromptTemplate.from_messages([
    ("human", "{input}"),
    ("ai", "{output}")
])

# 5. å®Œæ•´çš„ç¤ºä¾‹æç¤ºæ¨¡æ¿
few_shot_msg_prompt = FewShotChatMessagePromptTemplate(
    example_selector=selector,
    example_prompt=prompt_sample)

# 6. å®Œæ•´çš„å¯¹è¯æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "è¯·ç»™å‡ºä¸‹åˆ—è¯è¯­çš„åä¹‰è¯"),
    few_shot_msg_prompt,
    ("human", "{input}")
])

# 7. æ¨¡æ¿æ ¼å¼åŒ–
prompt = prompt.invoke({"input": "å¿§éƒ"})
print(prompt)
"""
messages=[
SystemMessage(content='è¯·ç»™å‡ºä¸‹åˆ—è¯è¯­çš„åä¹‰è¯', additional_kwargs={}, response_metadata={}), 
HumanMessage(content='é«˜å…´', additional_kwargs={}, response_metadata={}), 
AIMessage(content='æ‚²ä¼¤', additional_kwargs={}, response_metadata={}), 
HumanMessage(content='é˜³å…‰', additional_kwargs={}, response_metadata={}),
AIMessage(content='é˜´æš—', additional_kwargs={}, response_metadata={}), 
HumanMessage(content='å¿§éƒ', additional_kwargs={}, response_metadata={})]
"""

# 8. æ¨¡å‹è°ƒç”¨
client = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                    model=os.getenv("DS_MODEL"), max_tokens=30)

response = client.invoke(prompt)
print(response.content)  # å¼€æœ—
```



### Output Parser

è¯­è¨€æ¨¡å‹è¿”å›çš„å†…å®¹é€šå¸¸éƒ½æ˜¯å­—ç¬¦ä¸²çš„æ ¼å¼ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰ï¼Œä½†åœ¨å®é™…AIåº”ç”¨å¼€å‘è¿‡ç¨‹ä¸­ï¼Œå¾€å¾€å¸Œæœ›modelå¯ä»¥è¿”å›æ›´ç›´è§‚ã€æ›´æ ¼å¼åŒ–çš„å†…å®¹ï¼Œä»¥ç¡®ä¿åº”ç”¨èƒ½å¤Ÿé¡ºåˆ©è¿›è¡Œåç»­çš„é€»è¾‘å¤„ç†ã€‚æ­¤æ—¶ï¼ŒLangChainæä¾›çš„**è¾“å‡ºè§£æå™¨**å°±æ´¾ä¸Šç”¨åœºäº†ã€‚

è¾“å‡ºè§£æå™¨ï¼ˆOutput Parserï¼‰è´Ÿè´£è·å– LLM çš„è¾“å‡ºå¹¶å°†å…¶è½¬æ¢ä¸ºæ›´åˆé€‚çš„æ ¼å¼ã€‚è¿™åœ¨åº”ç”¨å¼€å‘ä¸­åŠå…¶é‡è¦ã€‚LangChainæœ‰è®¸å¤šä¸åŒç±»å‹çš„è¾“å‡ºè§£æå™¨ï¼š

- **StrOutputParser**ï¼šå­—ç¬¦ä¸²è§£æå™¨ 
- **JsonOutputParser**ï¼šJSONè§£æå™¨ï¼Œç¡®ä¿è¾“å‡ºç¬¦åˆç‰¹å®šJSONå¯¹è±¡æ ¼å¼ 
- **XMLOutputParser**ï¼šXMLè§£æå™¨ï¼Œå…è®¸ä»¥æµè¡Œçš„XMLæ ¼å¼ä»LLMè·å–ç»“æœ 
- **CommaSeparatedListOutputParser**ï¼šCSVè§£æå™¨ï¼Œæ¨¡å‹çš„è¾“å‡ºä»¥é€—å·åˆ†éš”ï¼Œä»¥åˆ—è¡¨å½¢å¼è¿”å›è¾“å‡º 
- **DatetimeOutputParser**ï¼šæ—¥æœŸæ—¶é—´è§£æå™¨ï¼Œå¯ç”¨äºå°† LLM è¾“å‡ºè§£æä¸ºæ—¥æœŸæ—¶é—´æ ¼å¼ 
- **EnumOutputParser**ï¼šæšä¸¾è§£æå™¨ï¼Œå°†LLMçš„è¾“å‡ºï¼Œè§£æä¸ºé¢„å®šä¹‰çš„æšä¸¾å€¼ 
- **StructuredOutputParser**ï¼šå°†éç»“æ„åŒ–æ–‡æœ¬è½¬æ¢ä¸ºé¢„å®šä¹‰æ ¼å¼çš„ç»“æ„åŒ–æ•°æ®ï¼ˆå¦‚å­—å…¸ï¼‰ 
- **OutputFixingParser**ï¼šè¾“å‡ºä¿®å¤è§£æå™¨ï¼Œç”¨äºè‡ªåŠ¨ä¿®å¤æ ¼å¼é”™è¯¯çš„è§£æå™¨ï¼Œæ¯”å¦‚å°†è¿”å›çš„ä¸ç¬¦åˆé¢„æœŸæ ¼å¼çš„è¾“å‡ºï¼Œå°è¯•ä¿®æ­£ä¸ºæ­£ç¡®çš„ç»“æ„åŒ–æ•°æ®ï¼ˆå¦‚ JSONï¼‰ 
- **RetryOutputParser**ï¼šé‡è¯•è§£æå™¨ï¼Œå½“ä¸»è§£æå™¨ï¼ˆå¦‚ JSONOutputParserï¼‰å› æ ¼å¼é”™è¯¯æ— æ³•è§£æLLM çš„è¾“å‡ºæ—¶ï¼Œé€šè¿‡è°ƒç”¨å¦ä¸€ä¸ªLLMè‡ªåŠ¨ä¿®æ­£é”™è¯¯ï¼Œå¹¶é‡æ–°å°è¯•è§£æ



#### StrOutputParser

StrOutputParser ç®€å•åœ°å°† ä»»ä½•è¾“å…¥ è½¬æ¢ä¸º å­—ç¬¦ä¸² ã€‚å®ƒæ˜¯ä¸€ä¸ªç®€å•çš„è§£æå™¨ï¼Œä»ç»“æœä¸­æå–contentå­—æ®µ

```PYTHON
import os
import dotenv

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# 1. å®Œæ•´çš„å¯¹è¯æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "è¯·ç»™å‡ºä¸‹åˆ—è¯è¯­çš„åä¹‰è¯"),
    ("human", "{input}")
])

# 2. æ¨¡æ¿æ ¼å¼åŒ–
prompt = prompt.invoke({"input": "å¿§éƒ"})


# 3. æ¨¡å‹è°ƒç”¨
client = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                    model=os.getenv("DS_MODEL"), max_tokens=30)
response = client.invoke(prompt)

# 4. ç»“æœè§£æ
parser = StrOutputParser()
result = parser.invoke(response)
print(type(result))  # <class 'str'>
print(result)  # å¿«ä¹ã€å¼€æœ—ã€æ¬¢å¿«ã€æ„‰æ‚¦ã€é«˜å…´
```



#### JsonOutputParser

JsonOutputParserï¼Œå³JSONè¾“å‡ºè§£æå™¨ï¼Œæ˜¯ä¸€ç§ç”¨äºå°†å¤§æ¨¡å‹çš„**è‡ªç”±æ–‡æœ¬è¾“å‡º**è½¬æ¢ä¸º ç»“æ„åŒ–JSONæ•°æ® çš„å·¥å…·ã€‚ 

**é€‚åˆåœºæ™¯**ï¼šç‰¹åˆ«é€‚ç”¨äºéœ€è¦ä¸¥æ ¼ç»“æ„åŒ–è¾“å‡ºçš„åœºæ™¯ï¼Œæ¯”å¦‚ API è°ƒç”¨ã€æ•°æ®å­˜å‚¨æˆ–ä¸‹æ¸¸ä»»åŠ¡å¤„ç†ã€‚

**å®ç°æ–¹å¼**

- æ–¹å¼1ï¼šç”¨æˆ·è‡ªå·±é€šè¿‡æç¤ºè¯æŒ‡æ˜è¿”å›Jsonæ ¼å¼ 
- æ–¹å¼2ï¼šå€ŸåŠ©JsonOutputParserçš„ get_format_instructions() ï¼Œç”Ÿæˆæ ¼å¼è¯´æ˜ï¼ŒæŒ‡å¯¼æ¨¡å‹è¾“å‡ºJSON ç»“æ„

æ–¹å¼1æ˜¯ç›®å‰ä¸»æµçš„ä½¿ç”¨æ–¹å¼ï¼Œå› ä¸ºç»“æœçš„åå¤„ç†ä¾èµ–å›ºå®šç»“æ„çš„Jsonä¸²ï¼Œæ–¹å¼2è¿”å›çš„ç»“æœæ¯ä¸€æ¬¡å¯èƒ½éƒ½ä¼šæœ‰å·®å¼‚ï¼Œå¯¹äºåå¤„ç†ä¸å‹å¥½ï¼Œä¸‹é¢æ¼”ç¤ºä¸€ä¸‹æ–¹å¼2ï¼š

```python
from langchain_core.output_parsers import JsonOutputParser

output_parser = JsonOutputParser()

# è¿”å›ä¸€äº›æŒ‡ä»¤æˆ–æ¨¡æ¿ï¼Œè¿™äº›æŒ‡ä»¤å‘Šè¯‰ç³»ç»Ÿå¦‚ä½•è§£ææˆ–æ ¼å¼åŒ–è¾“å‡ºæ•°æ®
format_instructions = output_parser.get_format_instructions()
print(format_instructions)  # Return a JSON object.
```

```PYTHON
import os
import dotenv

from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# 1. å®Œæ•´çš„å¯¹è¯æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "è¯·ç»™å‡ºä¸‹åˆ—è¯è¯­çš„åä¹‰è¯ï¼Œ {format_instructions}"),
    ("human", "{input}")
])

# 2. jsonè§£æå™¨
parser = JsonOutputParser()

# 3. æ¨¡æ¿æ ¼å¼åŒ–ã€‚æœ¬è´¨ä¸Šè¿˜æ˜¯åœ¨æ„å»ºprompt
prompt = prompt.invoke({"input": "å¿§éƒ", "format_instructions": parser.get_format_instructions()})

# 4. æ¨¡å‹è°ƒç”¨
client = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                    model=os.getenv("DS_MODEL"), max_tokens=30)
response = client.invoke(prompt)

# 4. ç»“æœè§£æ
result = parser.invoke(response)
print(type(result))  # <class 'dict'>
print(result)  # {'å¿§éƒ': 'å¿«ä¹'}
```



#### XMLOutputParser

XMLOutputParserï¼Œå°†æ¨¡å‹çš„è‡ªç”±æ–‡æœ¬è¾“å‡ºè½¬æ¢ä¸ºå¯ç¼–ç¨‹å¤„ç†çš„ XML æ•°æ®ã€‚

**å¦‚ä½•å®ç°**ï¼šåœ¨ PromptTemplate ä¸­æŒ‡å®š XML æ ¼å¼è¦æ±‚ï¼Œè®©æ¨¡å‹è¿”å› <tag>content</tag> å½¢å¼çš„æ•°æ®ã€‚ 

**æ³¨æ„**ï¼šXMLOutputParser ä¸ä¼šç›´æ¥å°†æ¨¡å‹çš„è¾“å‡ºä¿æŒä¸ºåŸå§‹XMLå­—ç¬¦ä¸²ï¼Œè€Œæ˜¯ä¼šè§£æXMLå¹¶è½¬æ¢æˆ Pythonå­—å…¸ ï¼ˆæˆ–ç±»ä¼¼ç»“æ„åŒ–çš„æ•°æ®ï¼‰ã€‚ç›®çš„æ˜¯ä¸ºäº†æ–¹ä¾¿ç¨‹åºåç»­å¤„ç†æ•°æ®ï¼Œè€Œä¸æ˜¯å•çº¯ä¿ç•™XMLæ ¼å¼ã€‚

```python
from langchain_core.output_parsers import XMLOutputParser

output_parser = XMLOutputParser()

format_instructions = output_parser.get_format_instructions()
print(format_instructions)
"""
The output should be formatted as a XML file.
1. Output should conform to the tags below.
2. If tags are not given, make them on your own.
3. Remember to always open and close all the tags.

As an example, for the tags ["foo", "bar", "baz"]:
1. String "<foo>
   <bar>
      <baz></baz>
   </bar>
</foo>" is a well-formatted instance of the schema.
2. String "<foo>
   <bar>
   </foo>" is a badly-formatted instance.
3. String "<foo>
   <tag>
   </tag>
</foo>" is a badly-formatted instance.

Here are the output tags:
â€‹```
None
â€‹```
"""
```



#### CommaSeparatedListOutputParser

åˆ—è¡¨è§£æå™¨ï¼šåˆ©ç”¨æ­¤è§£æå™¨å¯ä»¥å°†æ¨¡å‹çš„æ–‡æœ¬å“åº”è½¬æ¢ä¸ºä¸€ä¸ªç”¨ é€—å·åˆ†éš”çš„åˆ—è¡¨ï¼ˆList[str]ï¼‰ ã€‚

```python
import os
import dotenv

from langchain_core.output_parsers import CommaSeparatedListOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# 1. å®Œæ•´çš„å¯¹è¯æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "è¯·ç»™å‡ºä¸‹åˆ—è¯è¯­çš„åä¹‰è¯ï¼Œ {format_instructions}"),
    ("human", "{input}")
])

# 2. jsonè§£æå™¨
parser = CommaSeparatedListOutputParser()

# 3. æ¨¡æ¿æ ¼å¼åŒ–ã€‚æœ¬è´¨ä¸Šè¿˜æ˜¯åœ¨æ„å»ºprompt
prompt = prompt.invoke({"input": "å¿§éƒ", "format_instructions": parser.get_format_instructions()})

# 4. æ¨¡å‹è°ƒç”¨
client = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                    model=os.getenv("DS_MODEL"), max_tokens=30)
response = client.invoke(prompt)

# 4. ç»“æœè§£æ
result = parser.invoke(response)
print(type(result))  # <class 'list'>
print(result)  # ['å¿«ä¹', 'å¼€æœ—', 'æ„‰å¿«', 'é«˜å…´', 'ä¹è§‚']
```



#### DatetimeOutputParser

åˆ©ç”¨æ­¤è§£æå™¨å¯ä»¥ç›´æ¥å°†LLMè¾“å‡ºè§£æä¸ºæ—¥æœŸæ—¶é—´æ ¼å¼ã€‚ 

- get_format_instructions()ï¼š è·å–æ—¥æœŸè§£æçš„æ ¼å¼åŒ–æŒ‡ä»¤ï¼ŒæŒ‡ä»¤ä¸ºï¼šWrite a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'ã€‚ 
  - ä¸¾ä¾‹ï¼š1206-08-16T17:39:06.176399Z



## Chains

Chainï¼šé“¾ï¼Œç”¨äºå°†å¤šä¸ªç»„ä»¶ï¼ˆæç¤ºæ¨¡æ¿ã€LLMæ¨¡å‹ã€è®°å¿†ã€å·¥å…·ç­‰ï¼‰è¿æ¥èµ·æ¥ï¼Œå½¢æˆå¯å¤ç”¨çš„ å·¥ä½œæµ ï¼Œå®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚

Chain çš„æ ¸å¿ƒæ€æƒ³æ˜¯**é€šè¿‡ç»„åˆä¸åŒçš„æ¨¡å—åŒ–å•å…ƒï¼Œå®ç°æ¯”å•ä¸€ç»„ä»¶æ›´å¼ºå¤§çš„åŠŸèƒ½ã€‚**æ¯”å¦‚ï¼š 

- å°†**LLM**ä¸**Prompt Template**ï¼ˆæç¤ºæ¨¡æ¿ï¼‰ç»“åˆ 
- å°†**LLM**ä¸**è¾“å‡ºè§£æå™¨**ç»“åˆ 
- å°†**LLM**ä¸**å¤–éƒ¨æ•°æ®**ç»“åˆï¼Œä¾‹å¦‚ç”¨äºé—®ç­” 
- å°†**LLM**ä¸**é•¿æœŸè®°å¿†**ç»“åˆï¼Œä¾‹å¦‚ç”¨äºèŠå¤©å†å²è®°å½• 
- é€šè¿‡å°†**ç¬¬ä¸€ä¸ªLLM**çš„è¾“å‡ºä½œä¸º**ç¬¬äºŒä¸ªLLM**çš„è¾“å…¥ï¼Œ...ï¼Œå°†å¤šä¸ªLLMæŒ‰é¡ºåºç»“åˆåœ¨ä¸€èµ·



### LCEL

LangChainè¡¨è¾¾å¼è¯­è¨€ï¼ˆLCELï¼ŒLangChain Expression Languageï¼‰æ˜¯ä¸€ç§å£°æ˜å¼æ–¹æ³•ï¼Œå¯ä»¥è½»æ¾åœ°å°†å¤šä¸ªç»„ä»¶é“¾æ¥æˆAIå·¥ä½œæµã€‚å®ƒé€šè¿‡PythonåŸç”Ÿæ“ä½œç¬¦ï¼ˆ|ï¼‰å°†ç»„ä»¶è¿æ¥æˆå¯æ‰§è¡Œæµç¨‹ï¼Œæ˜¾è‘—ç®€åŒ–äº†AIåº”ç”¨çš„å¼€å‘ã€‚

**LCELçš„åŸºæœ¬æ„æˆï¼š**æç¤ºï¼ˆPromptï¼‰+ æ¨¡å‹ï¼ˆModelï¼‰+ è¾“å‡ºè§£æå™¨ï¼ˆOutputParserï¼‰

- **Promptï¼š**Prompt æ˜¯ä¸€ä¸ª BasePromptTemplateï¼Œè¿™æ„å‘³ç€å®ƒæ¥å—ä¸€ä¸ªæ¨¡æ¿å˜é‡çš„å­—å…¸å¹¶ç”Ÿæˆä¸€ä¸ª PromptValue ã€‚PromptValue å¯ä»¥ä¼ é€’ç»™ LLMï¼ˆå­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥ï¼‰æˆ– ChatModelï¼ˆæ¶ˆæ¯åºåˆ—ä½œä¸ºè¾“å…¥ï¼‰ã€‚ 

- **Modelï¼š**å°† PromptValue ä¼ é€’ç»™ modelã€‚å¦‚æœæˆ‘ä»¬çš„ model æ˜¯ä¸€ä¸ª ChatModelï¼Œè¿™æ„å‘³ç€å®ƒå°†è¾“å‡ºä¸€ä¸ª BaseMessage ã€‚ 

- **OutputParserï¼š**å°† model çš„è¾“å‡ºä¼ é€’ç»™ output_parserï¼Œå®ƒæ˜¯ä¸€ä¸ª BaseOutputParserï¼Œæ„å‘³ç€å®ƒå¯ä»¥æ¥å—å­—ç¬¦ä¸²æˆ– BaseMessage ä½œä¸ºè¾“å…¥ã€‚ 

- **chainï¼š**æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ | è¿ç®—ç¬¦è½»æ¾åˆ›å»ºè¿™ä¸ªChainã€‚ | è¿ç®—ç¬¦åœ¨ LangChain ä¸­ç”¨äºå°†ä¸¤ä¸ªå…ƒç´ ç»„åˆåœ¨ä¸€èµ·ã€‚
- **invokeï¼š**æ‰€æœ‰LCELå¯¹è±¡éƒ½å®ç°äº† Runnable åè®®ï¼Œä¿è¯ä¸€è‡´çš„è°ƒç”¨æ–¹å¼ ï¼ˆ invoke / batch / stream ï¼‰



Runnableæ˜¯LangChainå®šä¹‰çš„ä¸€ä¸ªæŠ½è±¡æ¥å£ï¼ˆProtocolï¼‰ï¼Œå®ƒ**å¼ºåˆ¶è¦æ±‚**æ‰€æœ‰LCELç»„ä»¶å®ç°ä¸€ç»„æ ‡å‡†æ–¹æ³•ï¼š

```python
class Runnable(Protocol):
    def invoke(self, input: Any) -> Any: ... # å•è¾“å…¥å•è¾“å‡º
    def batch(self, inputs: List[Any]) -> List[Any]: ... # æ‰¹é‡å¤„ç†
    def stream(self, input: Any) -> Iterator[Any]: ... # æµå¼è¾“å‡º
    # è¿˜æœ‰å…¶ä»–æ–¹æ³•å¦‚ ainvokeï¼ˆå¼‚æ­¥ï¼‰ç­‰...
```

ä»»ä½•å®ç°äº†è¿™äº›æ–¹æ³•çš„å¯¹è±¡éƒ½è¢«è§†ä¸ºLCELå…¼å®¹ç»„ä»¶ã€‚æ¯”å¦‚ï¼šèŠå¤©æ¨¡å‹ã€æç¤ºè¯æ¨¡æ¿ã€è¾“å‡ºè§£æå™¨ã€æ£€ç´¢å™¨ã€ä»£ç†(æ™ºèƒ½ä½“)ç­‰ã€‚ æ¯ä¸ª LCEL å¯¹è±¡éƒ½å®ç°äº† Runnable æ¥å£ï¼Œè¯¥æ¥å£å®šä¹‰äº†ä¸€ç»„å…¬å…±çš„è°ƒç”¨æ–¹æ³•ã€‚è¿™ä½¿å¾— LCEL å¯¹è±¡é“¾è‡ªåŠ¨æ”¯æŒè¿™äº›è°ƒç”¨æˆä¸ºå¯èƒ½ã€‚

ä¸‹é¢æŠŠå‰é¢çš„ç¤ºä¾‹ï¼Œä¿®æ”¹ä¸ºé“¾å¼å¯¹è±¡è¿›è¡Œè°ƒç”¨ï¼š

```python
import os
import dotenv

from langchain_core.output_parsers import CommaSeparatedListOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# 1. å®Œæ•´çš„å¯¹è¯æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "è¯·ç»™å‡ºä¸‹åˆ—è¯è¯­çš„åä¹‰è¯ï¼Œ {format_instructions}"),
    ("human", "{input}")
])

# 2. jsonè§£æå™¨
parser = CommaSeparatedListOutputParser()

# 3. LLM
llm = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                 model=os.getenv("DS_MODEL"), max_tokens=30)

# 4. åˆ›å»ºchain
chain = prompt | llm | parser

# 5. è°ƒç”¨chain
response = chain.invoke({"input": "å¿§éƒ", "format_instructions": parser.get_format_instructions()})

print(type(response))  # <class 'list'>
print(response)  # ['å¿«ä¹', 'å¼€æœ—', 'æ„‰å¿«', 'æ¬¢å¿«', 'ä¹è§‚']
```



### ä¼ ç»Ÿchain

åœ¨LCELä¹‹å‰ï¼ŒLangChainæä¾›äº†ä¸€äº›å¸¸ç”¨çš„é“¾ç±»å‹ã€‚



#### åŸºç¡€é“¾-LLMChain

è¿™ä¸ªé“¾è‡³å°‘åŒ…æ‹¬ä¸€ä¸ªæç¤ºè¯æ¨¡æ¿ï¼ˆPromptTemplateï¼‰ï¼Œä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼ˆLLM æˆ–èŠå¤©æ¨¡å‹ï¼‰ã€‚è¿™ä¸ªé“¾å·²ç»ä¸æ¨èä½¿ç”¨äº†ï¼Œåç»­æ–°ç‰ˆæœ¬å°†åºŸå¼ƒã€‚

**ç‰¹ç‚¹ï¼š**

- ç”¨äº**å•æ¬¡é—®ç­”**ï¼Œè¾“å…¥ä¸€ä¸ª Promptï¼Œè¾“å‡º LLM çš„å“åº”ã€‚ 
- é€‚åˆ**æ— ä¸Šä¸‹æ–‡**çš„ç®€å•ä»»åŠ¡ï¼ˆå¦‚ç¿»è¯‘ã€æ‘˜è¦ã€åˆ†ç±»ç­‰ï¼‰ã€‚ 
- **æ— è®°å¿†ï¼š**æ— æ³•è‡ªåŠ¨ç»´æŠ¤èŠå¤©å†å²

**ä½¿ç”¨æ­¥éª¤ï¼š**

1. **é…ç½®ä»»åŠ¡é“¾ï¼š**ä½¿ç”¨LLMChainç±»å°†ä»»åŠ¡ä¸æç¤ºè¯ç»“åˆï¼Œå½¢æˆå®Œæ•´çš„ä»»åŠ¡é“¾ã€‚

   ```python
   chain = LLMChain(llm = llm, prompt = prompt_template)
   ```

2. **æ‰§è¡Œä»»åŠ¡é“¾ï¼š**ä½¿ç”¨invoke()ç­‰æ–¹æ³•æ‰§è¡Œä»»åŠ¡é“¾ï¼Œå¹¶è·å–ç”Ÿæˆç»“æœã€‚å¯ä»¥æ ¹æ®éœ€è¦å¯¹è¾“å‡ºè¿›è¡Œå¤„ç†å’Œå±•ç¤ºã€‚

   ```python
   result = chain.invoke(...)
   ```



#### é¡ºåºé“¾-SimpleSequentialChain

é¡ºåºé“¾ï¼ˆSequentialChainï¼‰å…è®¸å°†å¤šä¸ªé“¾é¡ºåºè¿æ¥èµ·æ¥ï¼Œæ¯ä¸ªChainçš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªChainçš„è¾“å…¥ï¼Œå½¢æˆç‰¹å®šåœºæ™¯çš„æµæ°´çº¿ï¼ˆPipelineï¼‰ã€‚

**é¡ºåºé“¾æœ‰ä¸¤ç§ç±»å‹ï¼š **

- å•ä¸ªè¾“å…¥/è¾“å‡ºï¼šå¯¹åº” SimpleSequentialChain 

- å¤šä¸ªè¾“å…¥/è¾“å‡ºï¼šå¯¹åº” SequentialChain



SimpleSequentialChainï¼šæœ€ç®€å•çš„é¡ºåºé“¾ï¼Œå¤šä¸ªé“¾ ä¸²è”æ‰§è¡Œ ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½æœ‰ å•ä¸€ çš„è¾“å…¥å’Œè¾“å‡ºï¼Œä¸€ä¸ªæ­¥éª¤çš„è¾“å‡ºå°±æ˜¯ä¸‹ä¸€ä¸ªæ­¥éª¤çš„è¾“å…¥ï¼Œæ— éœ€æ‰‹åŠ¨æ˜ å°„ã€‚

<img src="/Users/seeker/Downloads/images/010.png" style='width: 75%'>

```python
import os
import dotenv
from langchain_classic.chains.llm import LLMChain
from langchain_classic.chains.sequential import SimpleSequentialChain
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# LLM
llm = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                 model=os.getenv("DS_MODEL"), max_tokens=30)

# ç¬¬ä¸€ä¸ªåŸºç¡€é“¾
prompt1 = ChatPromptTemplate.from_messages([
    ("system", "æ‚¨æ˜¯ä¸€ä½è¯—äºº"),
    ("human", "{input}")
])
chain1 = LLMChain(llm=llm, prompt=prompt1)

# ç¬¬äºŒä¸ªåŸºç¡€é“¾
prompt2 = ChatPromptTemplate.from_messages([
    ("system", "è¯·æŠŠä¸‹é¢çš„å¤è¯—ç¿»è¯‘æˆè‹±æ–‡"),
    ("human", "{poem}")
])
chain2 = LLMChain(llm=llm, prompt=prompt2)

# æ„å»ºé¡ºåºé“¾
ssc = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)

# è°ƒç”¨é“¾ï¼Œå› ä¸ºç®€å•é¡ºåºé“¾ç½®äºä¸€ä¸ªè¾“å…¥è¾“å‡ºï¼Œæ‰€ä»¥ç¬¬ä¸€ä¸ªåŸºç¡€é“¾çš„è¾“å‡ºå°†ç›´æ¥ä½œä¸ºç¬¬äºŒä¸ªåŸºç¡€é“¾çš„è¾“å…¥
response = ssc.invoke({"input": "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„äº”è¨€è¯—"})

print(type(response))  # <class 'dict'>
print(response)
"""
{'input': 'è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„äº”è¨€è¯—', 'output': "Spring Journey  \n\nGreen rills part the willows' hue,  
\nWhite birds pierce through misty view.  \nRain feeds moss on paths anew"}
"""
```



#### é¡ºåºé“¾-SequentialChain

SequentialChainï¼šæ›´é€šç”¨çš„é¡ºåºé“¾ï¼Œå…·ä½“æ¥è¯´ï¼š 

- **å¤šå˜é‡æ”¯æŒ**ï¼šå…è®¸ä¸åŒå­é“¾æœ‰ç‹¬ç«‹çš„è¾“å…¥/è¾“å‡ºå˜é‡ã€‚ 
- **çµæ´»æ˜ å°„**ï¼šéœ€**æ˜¾å¼å®šä¹‰**å˜é‡å¦‚ä½•ä»ä¸€ä¸ªé“¾ä¼ é€’åˆ°ä¸‹ä¸€ä¸ªé“¾ã€‚å³ç²¾å‡†åœ°å‘½åè¾“å…¥å…³é”®å­—å’Œè¾“å‡ºå…³é”®å­—ï¼Œæ¥æ˜ç¡®é“¾ä¹‹é—´çš„å…³ç³»ã€‚ 
- **å¤æ‚æµç¨‹æ§åˆ¶**ï¼šæ”¯æŒåˆ†æ”¯ã€æ¡ä»¶é€»è¾‘ï¼ˆåˆ†åˆ«é€šè¿‡ input_variables å’Œ output_variables é…ç½®è¾“å…¥å’Œè¾“å‡ºï¼‰ã€‚

<img src="/Users/seeker/Downloads/images/011.png" style='width: 80%'>

```python
import os
import dotenv
from langchain_classic.chains.llm import LLMChain
from langchain_classic.chains.sequential import SequentialChain
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# LLM
llm = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                 model=os.getenv("DS_MODEL"), max_tokens=200)

# ç¬¬ä¸€ä¸ªåŸºç¡€é“¾
prompt1 = ChatPromptTemplate.from_messages([("system", "æ‚¨æ˜¯ä¸€ä½è¯—äºº"), ("human", "{input}")])
chain1 = LLMChain(llm=llm, prompt=prompt1, output_key="poem")  # ç”³æ˜å½“å‰å­é“¾è¾“å‡ºç»“æœæ˜ å°„çš„key

# ç¬¬äºŒä¸ªåŸºç¡€é“¾ï¼Œå¼•ç”¨ç¬¬ä¸€ä¸ªå­é“¾çš„è¾“å‡º
prompt2 = ChatPromptTemplate.from_messages([("human", "è¯·æŠŠä¸‹é¢çš„å¤è¯—ç¿»è¯‘æˆè‹±æ–‡ï¼Œä¸éœ€è¦è¯‘æ–‡è¯´æ˜ï¼Œè¿”å›ç¿»è¯‘ç»“æœå³å¯ã€‚\n{poem}")])
chain2 = LLMChain(llm=llm, prompt=prompt2, output_key="en_poem")

# ç¬¬ä¸‰ä¸ªåŸºç¡€é“¾ï¼Œå¼•ç”¨ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªå­é“¾çš„è¾“å‡º
prompt3 = ChatPromptTemplate.from_messages([("human", "ä¸‹é¢è¯‘æ–‡æ˜¯å¯¹åŸæ–‡çš„ç¿»è¯‘ï¼Œè¯·ç»™ç¿»è¯‘çš„æ•ˆæœæ‰“åˆ†ï¼Œåˆ†å€¼èŒƒå›´: 0-100ã€‚æœ€åè¿”å›è¯„åˆ†å³å¯ã€‚\n"
                                                      "åŸæ–‡:\n{poem}\n\nè¯‘æ–‡:\n{en_poem}")])
chain3 = LLMChain(llm=llm, prompt=prompt3, output_key="score")

# æ„å»ºé¡ºåºé“¾
# å­é“¾çš„è¾“å‡ºå¼•ç”¨èƒ½å¤Ÿç”Ÿæ•ˆï¼Œä¾èµ–äºé¡ºåºé“¾å®šä¹‰è¾“å…¥è¾“å‡ºå˜é‡
ssc = SequentialChain(chains=[chain1, chain2, chain3], verbose=True,
                      input_variables=["input"], output_variables=["poem", "en_poem", "score"])

# è°ƒç”¨é“¾ï¼Œå› ä¸ºç®€å•é¡ºåºé“¾ç½®äºä¸€ä¸ªè¾“å…¥è¾“å‡ºï¼Œæ‰€ä»¥ç¬¬ä¸€ä¸ªåŸºç¡€é“¾çš„è¾“å‡ºå°†ç›´æ¥ä½œä¸ºç¬¬äºŒä¸ªåŸºç¡€é“¾çš„è¾“å…¥
response = ssc.invoke({"input": "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„äº”è¨€è¯—ï¼Œæœ€åè¿”å›è¯—å°±å¤Ÿäº†ï¼Œä¸éœ€è¦æœ‰é‰´èµã€èµæä¹‹ç±»çš„å†…å®¹"})

# å®šä¹‰çš„è¾“å…¥è¾“å‡ºå˜é‡åœ¨ç»“æœä¸­éƒ½å¯ä»¥è·å–åˆ°
print(response)
"""
{'input': 'è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„äº”è¨€è¯—ï¼Œæœ€åè¿”å›è¯—å°±å¤Ÿäº†ï¼Œä¸éœ€è¦æœ‰é‰´èµã€èµæä¹‹ç±»çš„å†…å®¹', 
'poem': 'ã€Šæ˜¥æ™“ã€‹\né£æš–æŸ³ä¸æ–œï¼Œ\næ–°æ¡ƒç»½ç²‰éœã€‚\nèºå•¼æ·±æ ‘é‡Œï¼Œ\nè¡”éœ²æ¶¦æ¢¨èŠ±ã€‚', 
'en_poem': '"Spring Dawn"  \nThe warm breeze tilts the willow strands,  
                            \nNew peach blossoms blush like dawn\'s hue.  
                            \nOrioles sing deep in the woods,  
                            \nDew-laden, they moisten pear blooms too.', 
'score': '95'}
"""
```



#### æ•°å­¦é“¾-LLMMathChain

LLMMathChainå°†ç”¨æˆ·é—®é¢˜è½¬æ¢ä¸ºæ•°å­¦é—®é¢˜ï¼Œç„¶åå°†æ•°å­¦é—®é¢˜è½¬æ¢ä¸ºå¯ä»¥ä½¿ç”¨ Python çš„ numexpr åº“æ‰§è¡Œçš„è¡¨è¾¾å¼ã€‚ä½¿ç”¨è¿è¡Œæ­¤ä»£ç çš„è¾“å‡ºæ¥å›ç­”é—®é¢˜ã€‚ä½¿ç”¨è¿™ä¸ªé“¾éœ€è¦å®‰è£… numexpr åº“ã€‚

```shell
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple/ numexpr
```

```python
import os
import dotenv
from langchain_classic.chains.llm_math.base import LLMMathChain
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# LLM
llm = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                 model=os.getenv("DS_MODEL"), max_tokens=200)

# åˆ›å»ºé“¾
llm_math = LLMMathChain.from_llm(llm)

# æ‰§è¡Œé“¾
res = llm_math.invoke("10 ** 3 + 100çš„ç»“æœæ˜¯å¤šå°‘ï¼Ÿ")
print(res)  # {'question': '10 ** 3 + 100çš„ç»“æœæ˜¯å¤šå°‘ï¼Ÿ', 'answer': 'Answer: 1100'}
```



#### è·¯ç”±é“¾-RouterChain

è·¯ç”±é“¾ï¼ˆRouterChainï¼‰ç”¨äºåˆ›å»ºå¯ä»¥ åŠ¨æ€é€‰æ‹©ä¸‹ä¸€æ¡é“¾ çš„é“¾ã€‚å¯ä»¥è‡ªåŠ¨åˆ†æç”¨æˆ·çš„éœ€æ±‚ï¼Œç„¶åå¼•å¯¼åˆ°æœ€é€‚åˆçš„é“¾ä¸­æ‰§è¡Œï¼Œè·å–å“åº”å¹¶è¿”å›æœ€ç»ˆç»“æœã€‚

æ¯”å¦‚ï¼Œæˆ‘ä»¬ç›®å‰æœ‰ä¸‰ç±»chainï¼Œåˆ†åˆ«å¯¹åº”ä¸‰ç§å­¦ç§‘çš„é—®é¢˜è§£ç­”ã€‚æˆ‘ä»¬çš„è¾“å…¥å†…å®¹ä¹Ÿæ˜¯ä¸è¿™ä¸‰ç§å­¦ç§‘å¯¹åº”ï¼Œä½†æ˜¯éšæœºçš„ï¼Œæ¯”å¦‚ç¬¬ä¸€æ¬¡è¾“å…¥æ•°å­¦é—®é¢˜ã€ç¬¬äºŒæ¬¡æœ‰å¯èƒ½æ˜¯å†å²é—®é¢˜... è¿™æ—¶å€™æœŸå¾…çš„æ•ˆæœæ˜¯ï¼šå¯ä»¥æ ¹æ®è¾“å…¥çš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Œè‡ªåŠ¨å°†å…¶åº”ç”¨åˆ°å¯¹åº”çš„å­é“¾ä¸­ã€‚RouterChainå°±ä¸ºæˆ‘ä»¬æä¾›äº†è¿™æ ·ä¸€ç§èƒ½åŠ›ã€‚

<img src="/Users/seeker/Downloads/images/012.png" style='width: 80%'>



#### æ–‡æ¡£é“¾-StuffDocumentsChain

StuffDocumentsChain æ˜¯ä¸€ç§æ–‡æ¡£å¤„ç†é“¾ï¼Œå®ƒçš„æ ¸å¿ƒä½œç”¨æ˜¯å°† å¤šä¸ªæ–‡æ¡£å†…å®¹åˆå¹¶ ï¼ˆâ€œå¡«å……â€æˆ–â€œå¡å…¥â€ï¼‰åˆ°å•ä¸ªæç¤ºï¼ˆpromptï¼‰ä¸­ï¼Œç„¶åä¼ é€’ç»™è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¤„ç†ã€‚ 

**ä½¿ç”¨åœºæ™¯**ï¼šç”±äºæ‰€æœ‰æ–‡æ¡£è¢«å®Œæ•´æ‹¼æ¥ï¼ŒLLM èƒ½åŒæ—¶çœ‹åˆ°å…¨éƒ¨å†…å®¹ï¼Œæ‰€ä»¥é€‚åˆéœ€è¦å…¨å±€ç†è§£çš„ä»»åŠ¡ï¼Œå¦‚æ€»ç»“ã€é—®ç­”ã€å¯¹æ¯”åˆ†æç­‰ã€‚ä½†æ³¨æ„ï¼Œä»…é€‚åˆå¤„ç†**å°‘é‡/ä¸­ç­‰é•¿åº¦æ–‡æ¡£**çš„åœºæ™¯ã€‚



### åŸºäºLCELçš„é“¾

çœ‹æœ€æ–°çš„åŸºäºLCELæ„å»ºçš„Chainsæœ‰å¦‚ä¸‹ä¸€äº›ï¼š

```shell
create_sql_query_chain 
create_stuff_documents_chain 
create_openai_fn_runnable 
create_structured_output_runnable 
load_query_constructor_runnable 
create_history_aware_retriever 
create_retrieval_chain
```

ä¸‹é¢æ¥ä»‹ç»ä¸€ä¸‹SQLæŸ¥è¯¢é“¾ï¼Œç”¨äºå°†**è‡ªç„¶è¯­è¨€**è½¬æ¢æˆ**æ•°æ®åº“çš„SQLæŸ¥è¯¢è¯­å¥**ï¼Œä½¿ç”¨è¿™ä¸ªé“¾éœ€è¦å®‰è£…pymysqlåº“ï¼š

```shell
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple/ pymysql
```

```python
import os
import dotenv

from langchain_classic.chains.sql_database.query import create_sql_query_chain
from langchain_community.utilities import SQLDatabase
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# LLM
llm = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                 model=os.getenv("DS_MODEL"), max_tokens=200)

# åˆ›å»ºdb
db_user = "root"
db_password = "your password"
db_host = "your db host"
db_port = "3306"
db_name = "xxx"

db = SQLDatabase.from_uri(f"mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}")

# æ„å»ºSQLæŸ¥è¯¢é“¾
csqc = create_sql_query_chain(llm=llm, db=db)

# åŸºäºè‡ªç„¶è¯­è¨€æ‰§è¡ŒæŸ¥è¯¢
resp1 = csqc.invoke({"question": "æŸ¥è¯¢deviceä¸­æœ‰å¤šå°‘ä¸ªandroidè®¾å¤‡"})
print(resp1)
"""
SELECT COUNT(*) AS android_device_count FROM `device` WHERE `platform` = 'android';
"""

# æˆ‘ä»¬ä¹Ÿå¯ä»¥æŒ‡å®šæŸ¥è¯¢çš„è¡¨
resp2 = csqc.invoke({"question": "æŸ¥è¯¢è¡¨ä¸­æœ‰å¤šå°‘ä¸ªandroidè®¾å¤‡", "table_names_to_use": ["device"]})
print(resp2)
"""
SQLQuery: SELECT COUNT(*) AS android_device_count FROM `device` WHERE `platform` = 'android';
"""
```



## Memory

**Memoryï¼Œæ˜¯LangChainä¸­ç”¨äºå¤šè½®å¯¹è¯ä¸­ä¿å­˜å’Œç®¡ç†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆæ¯”å¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰ï¼‰çš„ç»„ä»¶ã€‚**å®ƒè®©åº”ç”¨èƒ½å¤Ÿè®°ä½ç”¨æˆ·ä¹‹å‰è¯´äº†ä»€ä¹ˆï¼Œä»è€Œå®ç°å¯¹è¯çš„**ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›**ï¼Œä¸ºæ„å»ºçœŸæ­£æ™ºèƒ½å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é“¾å¼å¯¹è¯ç³»ç»Ÿæä¾›äº†åŸºç¡€ã€‚

è®¾è®¡ç†å¿µï¼š

<img src="/Users/seeker/Downloads/images/013.png" style='width: 100%'>

å¦‚æœChainé…ç½®äº†memoryï¼Œé‚£ä¹ˆæ¯ä¸€æ¬¡è°ƒç”¨ä¼šä¸memoryè¿›è¡Œä¸¤æ¬¡äº¤äº’ï¼Œè¯»å–å’Œå†™å…¥å„ä¸€æ¬¡ï¼š

1. æ”¶åˆ°ç”¨æˆ·è¾“å…¥æ—¶ï¼Œä»è®°å¿†ç»„ä»¶ä¸­æŸ¥è¯¢ç›¸å…³å†å²ä¿¡æ¯ï¼Œæ‹¼æ¥å†å²ä¿¡æ¯å’Œç”¨æˆ·çš„è¾“å…¥åˆ°æç¤ºè¯ä¸­ä¼ ç»™LLMã€‚ 
2. è¿”å›å“åº”ä¹‹å‰ï¼Œè‡ªåŠ¨æŠŠLLMè¿”å›çš„å†…å®¹å†™å…¥åˆ°è®°å¿†ç»„ä»¶ï¼Œç”¨äºä¸‹æ¬¡æŸ¥è¯¢ã€‚

ä¸ºäº†å¤„ç†ä¸åŒåœºæ™¯çš„ä¸Šä¸‹æ–‡å­˜å‚¨éœ€æ±‚ï¼ŒLangChainæ„å»ºäº†ä¸€äº›å¯ä»¥ç›´æ¥ä½¿ç”¨çš„ Memory å·¥å…·ï¼Œç”¨äºå­˜å‚¨èŠå¤©æ¶ˆæ¯çš„ä¸€ç³»åˆ—é›†æˆã€‚

<img src="/Users/seeker/Downloads/images/015.png" style='width: 100%'>



### äººå·¥å®ç°è®°å¿†

æˆ‘ä»¬å¯ä»¥å€ŸåŠ©æç¤ºè¯æ¨¡æ¿ä¸­çš„`messages`å±æ€§ï¼ˆä¿å­˜æ¶ˆæ¯çš„åˆ—è¡¨ï¼‰ï¼Œä¸æ–­å­˜å‚¨queryå’Œanswerï¼Œæ¥å®ç°è®°å¿†èƒ½åŠ›ã€‚

```python
import os
import dotenv

from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# LLM
llm = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                 model=os.getenv("DS_MODEL"), max_tokens=200)

chat_prompt_template = ChatPromptTemplate.from_messages([("system", "ä½ æ˜¯ä¸€ä½äººå·¥æ™ºèƒ½å°åŠ©æ‰‹ï¼Œä½ å«å°æ™º")])

# åˆ›å»ºé“¾
chain = chat_prompt_template | llm

# æ¨¡æ‹Ÿä¼šè¯
while True:
    query = input("è¯·è¾“å…¥ä½ çš„é—®é¢˜(é€€å‡º: quit):")
    
    if query == "quit":
        break

    # æŠŠçœŸå®é—®é¢˜ç»™åˆ°æ¨¡æ¿å¹¶è°ƒç”¨å¤§æ¨¡å‹
    chat_prompt_template.messages.append(HumanMessage(content=query))
    resp = chain.invoke({})
    
    # æŠŠAIå›ç­”ä¹Ÿä¿å­˜èµ·æ¥
    chat_prompt_template.messages.append(AIMessage(content=resp.content))
    print(chat_prompt_template.messages[-1].content)  # è¾“å‡ºaiå›ç­”
```

æ¨¡æ‹Ÿå¯¹è¯:

<img src="/Users/seeker/Downloads/images/014.png" style='width: 100%'>



### ChatMessageHistory

ChatMessageHistoryæ˜¯ä¸€ä¸ªç”¨äº**å­˜å‚¨å’Œç®¡ç†å¯¹è¯æ¶ˆæ¯**çš„åŸºç¡€ç±»ï¼Œå®ƒç›´æ¥æ“ä½œæ¶ˆæ¯å¯¹è±¡ï¼ˆå¦‚HumanMessage, AIMessage ç­‰ï¼‰ï¼Œæ˜¯å…¶å®ƒè®°å¿†ç»„ä»¶çš„åº•å±‚å­˜å‚¨å·¥å…·ã€‚

**ç‰¹ç‚¹ï¼š** 

- çº¯ç²¹æ˜¯æ¶ˆæ¯å¯¹è±¡çš„â€œ å­˜å‚¨å™¨ â€ï¼Œä¸è®°å¿†ç­–ç•¥ï¼ˆå¦‚ç¼“å†²ã€çª—å£ã€æ‘˜è¦ç­‰ï¼‰æ— å…³ã€‚ 
- ä¸æ¶‰åŠæ¶ˆæ¯çš„æ ¼å¼åŒ–ï¼ˆå¦‚è½¬æˆæ–‡æœ¬å­—ç¬¦ä¸²ï¼‰

è¿™å’Œæˆ‘ä»¬ä¸Šé¢æ‰‹åŠ¨å®ç°çš„è®°å¿†èƒ½åŠ›å¾ˆç›¸ä¼¼ï¼Œæˆ‘ä»¬æŠŠå‰é¢çš„ä»£ç ç¨ä½œæ”¹é€ ï¼š

```python
import os
import dotenv

from langchain_core.messages import SystemMessage
from langchain_openai import ChatOpenAI
from langchain_community.chat_message_histories import ChatMessageHistory

dotenv.load_dotenv()

# LLM
llm = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                 model=os.getenv("DS_MODEL"), max_tokens=200)

# ç›´æ¥åˆ›å»ºå¯¹è¯å†å²
history = ChatMessageHistory()
history.add_message(SystemMessage("ä½ æ˜¯ä¸€ä½äººå·¥æ™ºèƒ½å°åŠ©æ‰‹ï¼Œä½ å«å°æ™º"))

# æ¨¡æ‹Ÿä¼šè¯
while True:
    query = input("è¯·è¾“å…¥ä½ çš„é—®é¢˜(é€€å‡º: quit):")
    
    if query == "quit":
        break

    # æŠŠçœŸå®é—®é¢˜ç»™åˆ°æ¨¡æ¿å¹¶è°ƒç”¨å¤§æ¨¡å‹
    history.add_user_message(query)
    resp = llm.invoke(history.messages)  # æ³¨æ„è¿™é‡Œè¦ä¼ å…¥æ¶ˆæ¯åˆ—è¡¨
    
    # æŠŠAIå›ç­”ä¹Ÿä¿å­˜èµ·æ¥
    history.add_ai_message(resp.content)
    print(history.messages[-1].content)  # è¾“å‡ºaiå›ç­”
```



### ConversationBufferMemory

ConversationBufferMemoryæ˜¯ä¸€ä¸ªåŸºç¡€çš„**å¯¹è¯è®°å¿†ï¼ˆMemoryï¼‰ç»„ä»¶**ï¼Œä¸“é—¨ç”¨äºæŒ‰**åŸå§‹é¡ºåºå­˜å‚¨**å®Œæ•´çš„å¯¹è¯å†å²ã€‚ 

**é€‚ç”¨åœºæ™¯ï¼š**å¯¹è¯è½®æ¬¡è¾ƒå°‘ã€ä¾èµ–å®Œæ•´ä¸Šä¸‹æ–‡çš„åœºæ™¯ï¼ˆå¦‚ç®€å•çš„èŠå¤©æœºå™¨ï¼‰ 

**ç‰¹ç‚¹ï¼š **

- å®Œæ•´å­˜å‚¨å¯¹è¯å†å² 
- ç®€å• ã€ æ— è£å‰ª ã€ æ— å‹ç¼©
- ä¸ Chains/Models æ— ç¼é›†æˆ 
- æ”¯æŒä¸¤ç§è¿”å›æ ¼å¼ï¼ˆé€šè¿‡ return_messages å‚æ•°æ§åˆ¶è¾“å‡ºæ ¼å¼ï¼‰ 
  - return_messages=True è¿”å›æ¶ˆæ¯å¯¹è±¡åˆ—è¡¨ï¼ˆ**List[BaseMessage]** ï¼‰
  - return_messages=False ï¼ˆé»˜è®¤ï¼‰ è¿”å›æ‹¼æ¥çš„**çº¯æ–‡æœ¬å­—ç¬¦ä¸²**

```python
import os
import dotenv
from langchain_classic.chains.llm import LLMChain
from langchain_classic.memory import ConversationBufferMemory

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# LLM
llm = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                 model=os.getenv("DS_MODEL"), max_tokens=200)

# åˆ›å»ºmemory
# return_messages è¡¨ç¤ºè¿”å›æ¶ˆæ¯ç±»å‹çš„åˆ—è¡¨è€Œä¸æ˜¯å­—ç¬¦ä¸²
# memory_key æ˜¯å­˜å‚¨åœ¨memoryä¸­çš„å†å²è®°å½•çš„é”®å
memory = ConversationBufferMemory(return_messages=True, memory_key="records")

# åˆ›å»ºprompt
# ä¸‹é¢æ’å…¥memoryä¸­çš„è®°å½•ï¼Œå˜é‡åè¦å’Œmemory_keyä¸€è‡´ã€‚memory_keyé»˜è®¤æ˜¯history
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¸äººç±»å¯¹è¯çš„æœºå™¨äººã€‚"),
    # MessagesPlaceholder(variable_name='history'),
    MessagesPlaceholder(variable_name='records'),
    ("human", "é—®é¢˜ï¼š{question}")
])

# åˆ›å»ºchain
chain = LLMChain(llm=llm, prompt=prompt, memory=memory)

# ç¬¬ä¸€æ¬¡è°ƒç”¨
print(memory.buffer_as_str)  # ç¬¬ä¸€æ¬¡æ²¡æœ‰è®°å½•
resp1 = chain.invoke({"question": "ä¸­å›½é¦–éƒ½åœ¨å“ªé‡Œï¼Ÿ"})
print(resp1)
"""
{'question': 'ä¸­å›½é¦–éƒ½åœ¨å“ªé‡Œï¼Ÿ', 
'records': [HumanMessage(content='ä¸­å›½é¦–éƒ½åœ¨å“ªé‡Œï¼Ÿ', additional_kwargs={}, response_metadata={}), 
            AIMessage(content='ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ã€‚åŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»ã€æ–‡åŒ–ã€å›½é™…äº¤å¾€å’Œç§‘æŠ€åˆ›æ–°ä¸­å¿ƒï¼Œæ‹¥æœ‰æ‚ ä¹…çš„å†å²å’Œä¸°å¯Œçš„æ–‡åŒ–é—äº§ã€‚', 
            additional_kwargs={}, response_metadata={})], 
'text': 'ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ã€‚åŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»ã€æ–‡åŒ–ã€å›½é™…äº¤å¾€å’Œç§‘æŠ€åˆ›æ–°ä¸­å¿ƒï¼Œæ‹¥æœ‰æ‚ ä¹…çš„å†å²å’Œä¸°å¯Œçš„æ–‡åŒ–é—äº§ã€‚'}
"""

# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼Œ
print(memory.buffer_as_str)
"""
Human: ä¸­å›½é¦–éƒ½åœ¨å“ªé‡Œï¼Ÿ
AI: ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ã€‚åŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»ã€æ–‡åŒ–ã€å›½é™…äº¤å¾€å’Œç§‘æŠ€åˆ›æ–°ä¸­å¿ƒï¼Œæ‹¥æœ‰æ‚ ä¹…çš„å†å²å’Œä¸°å¯Œçš„æ–‡åŒ–é—äº§ã€‚
"""

resp2 = chain.invoke({"question": "æˆ‘åˆšæ‰é—®äº†ä»€ä¹ˆé—®é¢˜"})
print(resp2)
"""
{'question': 'æˆ‘åˆšæ‰é—®äº†ä»€ä¹ˆé—®é¢˜', 
'records': [HumanMessage(content='ä¸­å›½é¦–éƒ½åœ¨å“ªé‡Œï¼Ÿ', additional_kwargs={}, response_metadata={}), 
            AIMessage(content='ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ã€‚åŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»ã€æ–‡åŒ–ã€å›½é™…äº¤å¾€å’Œç§‘æŠ€åˆ›æ–°ä¸­å¿ƒï¼Œæ‹¥æœ‰æ‚ ä¹…çš„å†å²å’Œä¸°å¯Œçš„æ–‡åŒ–é—äº§ã€‚', 
            additional_kwargs={}, response_metadata={}), 
            HumanMessage(content='æˆ‘åˆšæ‰é—®äº†ä»€ä¹ˆé—®é¢˜', additional_kwargs={}, response_metadata={}), 
            AIMessage(content='ä½ åˆšæ‰çš„é—®é¢˜æ˜¯ï¼šâ€œä¸­å›½é¦–éƒ½åœ¨å“ªé‡Œï¼Ÿâ€', additional_kwargs={}, response_metadata={})], 
'text': 'ä½ åˆšæ‰çš„é—®é¢˜æ˜¯ï¼šâ€œä¸­å›½é¦–éƒ½åœ¨å“ªé‡Œï¼Ÿâ€'}
"""
```



### ConversationChain

ConversationChainå®é™…ä¸Šæ˜¯å°±æ˜¯å¯¹**ConversationBufferMemory**å’Œ**LLMChain**è¿›è¡Œäº†å°è£…ï¼Œå¹¶ä¸”æä¾›ä¸€ä¸ªé»˜è®¤æ ¼å¼çš„æç¤ºè¯æ¨¡ç‰ˆï¼ˆæˆ‘ä»¬ä¹Ÿå¯ä»¥ä¸ç”¨ï¼‰ï¼Œä»è€Œç®€åŒ–äº†åˆå§‹åŒ–ConversationBufferMemoryçš„æ­¥éª¤ã€‚

ConversationChainè¿˜æä¾›äº†å†…ç½®æ¨¡æ¿ï¼Œç®€å•å¯¹è¯åœºæ™¯ä¸‹å¯å¿«é€Ÿå¼€å§‹ï¼Œå†…éƒ¨åŒ…å« inputã€history ä¸¤ä¸ªå˜é‡ã€‚

```python
import os
import dotenv

from langchain_classic.chains.conversation.base import ConversationChain
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# LLM
llm = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                 model=os.getenv("DS_MODEL"), max_tokens=200)

# åˆ›å»ºé“¾
chain = ConversationChain(llm=llm)

# æ‰§è¡Œé“¾
result1 = chain.invoke({"input": "å°æ˜æœ‰1åªçŒ«ï¼Œå°åˆšå…»äº†2åªç‹—ï¼Œä»–ä»¬éƒ½å¥½æœ‰çˆ±å¿ƒå‘€ï¼Œä½ æœ‰çˆ±å¿ƒå—"})
result2 = chain.invoke({"input": "å°æ˜å’Œå°åˆšå…»äº†å¤šå°‘åªå® ç‰©ï¼Ÿ"})
print(result2["response"])
"""
å°æ˜å’Œå°åˆšä¸€å…±å…»äº†3åªå® ç‰©å“¦ï¼å…·ä½“æ¥è¯´ï¼š
- å°æ˜æœ‰1åªçŒ« ğŸ±
- å°åˆšæœ‰2åªç‹— ğŸ¶
åŠ èµ·æ¥å°±æ˜¯1 + 2 = 3åªå® ç‰©å•¦ï¼ä»–ä»¬çœŸæ˜¯æœ‰çˆ±çš„ç»„åˆå‘¢ï½ ğŸŒŸ
"""
```



### ConversationBufferWindowMemory

åœ¨äº†è§£äº†ConversationBufferMemoryè®°å¿†ç±»åï¼Œæˆ‘ä»¬çŸ¥é“äº†å®ƒèƒ½å¤Ÿæ— é™çš„å°†å†å²å¯¹è¯ä¿¡æ¯å¡«å……åˆ°Historyä¸­ï¼Œä»è€Œç»™å¤§æ¨¡å‹æä¾›ä¸Šä¸‹æ–‡çš„èƒŒæ™¯ã€‚ä½†è¿™ä¼š**å¯¼è‡´å†…å­˜é‡ååˆ†å¤§**ï¼Œå¹¶ä¸”**æ¶ˆè€—çš„tokenæ˜¯éå¸¸å¤š**çš„ï¼Œæ­¤å¤–ï¼Œæ¯ä¸ªå¤§æ¨¡å‹éƒ½å­˜åœ¨æœ€å¤§è¾“å…¥çš„Tokené™åˆ¶ã€‚ 

æˆ‘ä»¬å‘ç°ï¼Œè¿‡ä¹…è¿œçš„å¯¹è¯æ•°æ®å¾€å¾€å¹¶ä¸èƒ½å¯¹å½“å‰è½®æ¬¡çš„é—®ç­”æä¾›æœ‰æ•ˆçš„ä¿¡æ¯ï¼ŒLangChain ç»™å‡ºçš„è§£å†³æ–¹å¼æ˜¯ï¼š**ConversationBufferWindowMemory**æ¨¡å—ã€‚è¯¥è®°å¿†ç±»ä¼š**ä¿å­˜ä¸€æ®µæ—¶é—´å†…å¯¹è¯äº¤äº’**çš„åˆ—è¡¨ï¼Œ ä»…ä½¿ç”¨æœ€è¿‘ K ä¸ªäº¤äº’ ã€‚è¿™æ ·å°±ä½¿ç¼“å­˜åŒºä¸ä¼šå˜å¾—å¤ªå¤§ã€‚ 

**ç‰¹ç‚¹ï¼š** 

- é€‚åˆé•¿å¯¹è¯åœºæ™¯ã€‚ 
- ä¸ Chains/Models æ— ç¼é›†æˆ 
- æ”¯æŒä¸¤ç§è¿”å›æ ¼å¼ï¼ˆé€šè¿‡ return_messages å‚æ•°æ§åˆ¶è¾“å‡ºæ ¼å¼ï¼‰ 
  - return_messages=True è¿”å›æ¶ˆæ¯å¯¹è±¡åˆ—è¡¨ï¼ˆ**List[BaseMessage]** ï¼‰
  - return_messages=False ï¼ˆé»˜è®¤ï¼‰ è¿”å›æ‹¼æ¥çš„**çº¯æ–‡æœ¬å­—ç¬¦ä¸²**

å…¶ä¸»è¦çš„å˜åŒ–å°±æ˜¯åœ¨å®ä¾‹åŒ–memoryæ—¶ï¼Œå¯ä»¥é…ç½®æŒ‡å®šä¸ªæ•°çš„æ¶ˆæ¯å¯¹ï¼ˆå‚æ•°Kï¼‰ã€‚

```python
import os
import dotenv
from langchain_classic.chains.llm import LLMChain
from langchain_classic.memory import ConversationBufferWindowMemory

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# LLM
llm = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                 model=os.getenv("DS_MODEL"), max_tokens=200)

# k è®°å½•è¿‘kç»„å¯¹è¯å†å²
memory = ConversationBufferWindowMemory(return_messages=True, k=1)

# åˆ›å»ºprompt
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¸äººç±»å¯¹è¯çš„æœºå™¨äººã€‚"),
    MessagesPlaceholder(variable_name='history'),
    ("human", "é—®é¢˜ï¼š{question}")
])

# åˆ›å»ºchain
chain = LLMChain(llm=llm, prompt=prompt, memory=memory)

# ç¬¬ä¸€æ¬¡è°ƒç”¨
chain.invoke({"question": "ä¸­å›½é¦–éƒ½åœ¨å“ªé‡Œï¼Ÿ"})

# ç¬¬äºŒæ¬¡è°ƒç”¨
chain.invoke({"question": "ä¸­å›½æœ‰å¤šå°‘ä¸ªçœä»½ï¼Ÿ"})

# ç¬¬ä¸‰æ¬¡è°ƒç”¨
resp = chain.invoke({"question": "æˆ‘é—®è¿‡ä¸­å›½é¦–éƒ½ç›¸å…³çš„é—®é¢˜å—"})
print(resp["text"])  # ç›®å‰ï¼Œåœ¨æˆ‘ä»¬çš„å¯¹è¯è®°å½•ä¸­ï¼Œ**æ²¡æœ‰**å…³äºä¸­å›½é¦–éƒ½çš„æé—®å†å²ã€‚ä¸è¿‡ï¼Œå¦‚æœæ‚¨æƒ³äº†è§£...
```



### ConversationTokenBufferMemory

ConversationTokenBufferMemory æ˜¯ LangChain ä¸­ä¸€ç§åŸºäº**Token æ•°é‡æ§åˆ¶**çš„å¯¹è¯è®°å¿†æœºåˆ¶ã€‚å¦‚æœå­—ç¬¦æ•°é‡è¶…å‡ºæŒ‡å®šæ•°ç›®ï¼Œå®ƒä¼šåˆ‡æ‰è¿™ä¸ªå¯¹è¯çš„æ—©æœŸéƒ¨åˆ†ï¼Œä»¥ä¿ç•™ä¸æœ€è¿‘çš„äº¤æµç›¸å¯¹åº”çš„å­—ç¬¦æ•°é‡ã€‚

ä½¿ç”¨æ—¶é€šè¿‡ max_token_limit å‚æ•°æ§åˆ¶tokenä¸Šé™ã€‚



### ConversationSummaryMemory

> åˆ°ç›®å‰ä¸ºæ­¢æ‰€äº†è§£çš„è®°å¿†ç±»å‹ï¼Œéƒ½å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯åŸå°ä¸åŠ¨çš„è®°å½•å¯¹è¯å†…å®¹ï¼Œå³ä¾¿é™åˆ¶**å¯¹è¯æ¡æ•°**è¿˜æ˜¯**token**éƒ½ä¸èƒ½æ—¢èŠ‚çœå‘¢å“¦æ‘åˆä¿è¯å¯¹è¯è´¨é‡ã€‚

ConversationSummaryMemoryæ˜¯ LangChain ä¸­ä¸€ç§**æ™ºèƒ½å‹ç¼©å¯¹è¯å†å²**çš„è®°å¿†æœºåˆ¶ï¼Œå®ƒé€šè¿‡å¤§è¯­è¨€æ¨¡å‹(LLM)è‡ªåŠ¨ç”Ÿæˆå¯¹è¯å†…å®¹çš„**ç²¾ç®€æ‘˜è¦**ï¼Œè€Œä¸æ˜¯å­˜å‚¨åŸå§‹å¯¹è¯æ–‡æœ¬ã€‚

è¿™ç§è®°å¿†æ–¹å¼ç‰¹åˆ«é€‚åˆ**é•¿å¯¹è¯**å’Œ**éœ€è¦ä¿ç•™æ ¸å¿ƒä¿¡æ¯**çš„åœºæ™¯ã€‚ 

**ç‰¹ç‚¹ï¼š **

- æ‘˜è¦ç”Ÿæˆ 
- åŠ¨æ€æ›´æ–° 
- ä¸Šä¸‹æ–‡ä¼˜åŒ–



### ConversationSummaryBufferMemory

ConversationSummaryBufferMemory æ˜¯ LangChainä¸­ä¸€ç§**æ··åˆå‹è®°å¿†æœºåˆ¶**ï¼Œå®ƒç»“åˆäº† ConversationBufferMemoryï¼ˆå®Œæ•´å¯¹è¯è®°å½•ï¼‰å’Œ ConversationSummaryMemoryï¼ˆæ‘˜è¦è®°å¿†ï¼‰çš„ä¼˜ç‚¹ï¼Œåœ¨ä¿ç•™æœ€è¿‘**å¯¹è¯åŸå§‹è®°å½•**çš„åŒæ—¶ï¼Œå¯¹è¾ƒæ—©çš„å¯¹è¯å†…å®¹è¿›è¡Œ**æ™ºèƒ½æ‘˜è¦**ã€‚ 

**ç‰¹ç‚¹ï¼š** 

- ä¿ç•™æœ€è¿‘Næ¡åŸå§‹å¯¹è¯ï¼šç¡®ä¿æœ€æ–°äº¤äº’çš„å®Œæ•´ä¸Šä¸‹æ–‡
- æ‘˜è¦è¾ƒæ—©å†å²ï¼šå¯¹è¶…å‡ºç¼“å†²åŒºçš„æ—§å¯¹è¯è¿›è¡Œå‹ç¼©ï¼Œé¿å…ä¿¡æ¯è¿‡è½½
- å¹³è¡¡ç»†èŠ‚ä¸æ•ˆç‡ï¼šæ—¢ä¸ä¼šä¸¢å¤±å…³é”®ç»†èŠ‚ï¼Œåˆèƒ½å¤„ç†é•¿å¯¹è¯

è¦å®ç°æ™ºèƒ½æ‘˜è¦ï¼Œé‚£ä¹ˆmemoryä¹Ÿéœ€è¦é…ç½®llmäº†ï¼Œç”¨äºæ‘˜è¦å¯¹è¯ä¿¡æ¯ã€‚å¦å¤–ï¼Œå’Œå‰é¢çš„è®°å¿†ç±»å‹ä¸€æ ·ï¼Œå¯ä»¥æ‰‹åŠ¨æ·»åŠ å·²æœ‰çš„å¯¹è¯è®°å½•ã€‚

```python
import os
import dotenv

from langchain_classic.memory import ConversationSummaryBufferMemory
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# LLM
llm = ChatOpenAI(api_key=os.getenv("API_KEY"), base_url=os.getenv("API_BASE"),
                 model="gpt-4o-mini")

# max_token_limit è¶…å‡ºtokené™åˆ¶çš„éƒ¨åˆ†æå–æ‘˜è¦
memory1 = ConversationSummaryBufferMemory(llm=llm, max_token_limit=32, return_messages=True)

# æ·»åŠ å·²æœ‰å¯¹è¯ä¿¡æ¯
# inputs ä¸­ä¸ç®¡keyæ˜¯ä»€ä¹ˆéƒ½æŠŠå†…å®¹è½¬æˆhumanä¿¡æ¯ï¼Œoutputsè½¬ä¸ºaiä¿¡æ¯
memory1.save_context(inputs={"input": "ä½ å¥½ï¼Œæˆ‘çš„åå­—å«å°æ˜"}, outputs={"output": "å¾ˆé«˜å…´è®¤è¯†ä½ ï¼Œå°æ˜"})
memory1.save_context({"input": "æç™½æ˜¯å“ªä¸ªæœä»£çš„è¯—äºº"}, {"output": "æç™½æ˜¯å”æœè¯—äºº"})
memory1.save_context({"input": "å”å®‹å…«å¤§å®¶é‡Œæœ‰è‹è½¼å—ï¼Ÿ"}, {"output": "æœ‰"})

print(memory1.load_memory_variables({}))
"""
{'history': [
SystemMessage(content='The human introduces himself as Xiaoming, to which the AI responds that it is pleased to meet him. The human then asks which dynasty the poet Li Bai belonged to, and the AI replies that Li Bai is a poet from the Tang Dynasty.', additional_kwargs={}, response_metadata={}), 
HumanMessage(content='å”å®‹å…«å¤§å®¶é‡Œæœ‰è‹è½¼å—ï¼Ÿ', additional_kwargs={}, response_metadata={}), 
AIMessage(content='æœ‰', additional_kwargs={}, response_metadata={})]}
"""
print(memory1.chat_memory.messages)
"""
[HumanMessage(content='å”å®‹å…«å¤§å®¶é‡Œæœ‰è‹è½¼å—ï¼Ÿ', additional_kwargs={}, response_metadata={}), 
AIMessage(content='æœ‰', additional_kwargs={}, response_metadata={})]
"""
```

ç»“åˆchainå®ç°ä¸€ä¸ªå®¢æœå¯¹è¯ï¼š

```python
import os
import dotenv
from langchain_classic.chains.llm import LLMChain
from langchain_classic.memory import ConversationSummaryBufferMemory

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

# åŒºåˆ†æ‘˜è¦æå–å’Œå®é™…å¯¹è¯çš„å¤§æ¨¡å‹
summary_llm = ChatOpenAI(api_key=os.getenv("API_KEY"), base_url=os.getenv("API_BASE"),
                         model="gpt-4o-mini")

chat_llm = ChatOpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_BASE_URL"),
                      model=os.getenv("DS_MODEL"), max_tokens=200)

# å¸¦æ‘˜è¦ç¼“å†²çš„è®°å¿†
memory = ConversationSummaryBufferMemory(llm=summary_llm, max_token_limit=100, return_messages=True,
                                         memory_key="chat_history")

# åˆ›å»ºprompt
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ç”µå•†å®¢æœåŠ©æ‰‹ï¼Œç”¨ä¸­æ–‡å‹å¥½å›å¤ç”¨æˆ·é—®é¢˜ã€‚ä¿æŒä¸“ä¸šä½†äº²åˆ‡çš„è¯­æ°”ã€‚"),
    MessagesPlaceholder(variable_name='chat_history'),
    ("human", "{query}")
])

# åˆ›å»ºchain
chain = LLMChain(llm=chat_llm, prompt=prompt, memory=memory)

# æ¨¡æ‹Ÿå¯¹è¯
texts = ["ä½ å¥½ï¼Œæˆ‘æƒ³æŸ¥è¯¢è®¢å•12345çš„çŠ¶æ€",
         "è¿™ä¸ªè®¢å•æ˜¯ä¸Šå‘¨äº”ä¸‹çš„",
         "æˆ‘ç°åœ¨æ€¥ç€ç”¨ï¼Œèƒ½åŠ æ€¥å¤„ç†å—",
         "ç­‰ç­‰ï¼Œæˆ‘å¯èƒ½è®°é”™è®¢å•å·äº†ï¼Œåº”è¯¥æ˜¯12346",
         "å¯¹äº†ï¼Œä½ ä»¬é€€è´§æ”¿ç­–æ˜¯æ€æ ·çš„"]

for text in texts:
    resp = chain.invoke({"query": text})
    print(resp["text"])  # aiå›ç­”

# æœ€åçš„è®°å¿†å†…å®¹
print(memory.load_memory_variables({}))
"""
{'chat_history': [
SystemMessage(content="The human requests the status of order 12345, and the AI confirms it has shipped via SF Express, providing tracking options and asking if the human needs further information. The human realizes they may have given the wrong order number, correcting it to 12346. The AI acknowledges this and checks order 12346, confirming it has also been shipped via ZTO Express with a new tracking number. The AI reiterates delivery expectations and offers assistance in tracking logistics, acknowledging the human's urgency. It explains that while the order is in transit and cannot be expedited directly, the human can contact ZTO for priority delivery if not received by tomorrow. The AI inquires which form of assistance the human prefers and reminds them that the information for order 12345 is still valid if needed. The human then asks about the return policy, and the AI responds with details about the return time frame, conditions, and special notes regarding shipping costs. The AI also reminds the human that order 12346 is still in transit and that they need to accept the package before initiating a return, offering further assistance with the return process if needed.", additional_kwargs={}, response_metadata={})]}
"""
```

è¿™é‡Œæœ€åä¸€ä¸ªé—®é¢˜å›ç­”å†…å®¹æœ‰ç‚¹å¤šï¼Œæ‰€ä»¥æŠŠå…¨éƒ¨å¯¹è¯éƒ½è¿›è¡Œæ‘˜è¦äº†



### ConversationEntityMemory

ConversationEntityMemory æ˜¯ä¸€ç§**åŸºäºå®ä½“çš„å¯¹è¯è®°å¿†æœºåˆ¶**ï¼Œå®ƒèƒ½å¤Ÿæ™ºèƒ½åœ°è¯†åˆ«ã€å­˜å‚¨å’Œåˆ©ç”¨å¯¹è¯ä¸­å‡ºç°çš„å®ä½“ä¿¡æ¯ï¼ˆå¦‚äººåã€åœ°ç‚¹ã€äº§å“ç­‰ï¼‰åŠå…¶ **å±æ€§/å…³ç³»**ï¼Œå¹¶ç»“æ„åŒ–å­˜å‚¨ï¼Œä½¿ AI å…·å¤‡æ›´å¼ºçš„ä¸Šä¸‹æ–‡ç†è§£å’Œè®°å¿†èƒ½åŠ›ã€‚ 

**å¥½å¤„ï¼šè§£å†³ä¿¡æ¯è¿‡è½½é—®é¢˜**

- é•¿å¯¹è¯ä¸­å¤§é‡å†—ä½™ä¿¡æ¯ä¼šå¹²æ‰°å…³é”®äº‹å®è®°å¿† 
- é€šè¿‡å¯¹å®ä½“æ‘˜è¦ï¼Œå¯ä»¥å‹ç¼©éé‡è¦ç»†èŠ‚ï¼ˆå¦‚åˆ é™¤å¯’æš„ç­‰ï¼Œä¿ç•™ä»·æ ¼/æ—¶é—´ç­‰ç¡¬æ€§äº‹å®ï¼‰ 

**åº”ç”¨åœºæ™¯ï¼š**åœ¨åŒ»ç–—ç­‰é«˜é£é™©é¢†åŸŸï¼Œå¿…é¡»ç”¨å®ä½“è®°å¿†ç¡®ä¿å…³é”®ä¿¡æ¯ï¼ˆå¦‚è¿‡æ•å²ï¼‰è¢«100%å‡†ç¡®è¯†åˆ«å’Œæ‹¦æˆªã€‚

ä¸‹é¢ä¸¾ä¸ªä¾‹å­ï¼Œæœ‰ä»¥ä¸‹å¯¹è¯å†…å®¹åˆ†åˆ«ä½¿ç”¨ä¸åŒçš„è®°å¿†ç±»è¿›è¡Œæ‘˜è¦æå–ï¼š

```shell
{"input": "æˆ‘å¤´ç—›ï¼Œè¡€å‹140/90ï¼Œåœ¨åƒé˜¿å¸åŒ¹æ—ã€‚"},
{"output": "å»ºè®®ç›‘æµ‹è¡€å‹ï¼Œé˜¿å¸åŒ¹æ—å¯ç»§ç»­æœç”¨ã€‚"}
{"input": "æˆ‘å¯¹é’éœ‰ç´ è¿‡æ•ã€‚"},
{"output": "å·²è®°å½•æ‚¨çš„é’éœ‰ç´ è¿‡æ•å²ã€‚"}
{"input": "é˜¿å¸åŒ¹æ—åƒäº†ä¸‰å¤©ï¼Œå¤´ç—›æ²¡ç¼“è§£ã€‚"},
{"output": "å»ºè®®åœç”¨é˜¿å¸åŒ¹æ—ï¼Œæ¢å¸ƒæ´›èŠ¬è¯•è¯•ã€‚"}
```

ä½¿ç”¨ConversationSummaryMemory:

```shell
"æ‚£è€…ä¸»è¯‰å¤´ç—›å’Œâ¾¼â¾å‹ï¼ˆ140/90ï¼‰ï¼Œæ­£åœ¨æœâ½¤é˜¿å¸åŒ¹æ—ã€‚æ‚£è€…å¯¹â»˜éœ‰ç´ è¿‡æ•ã€‚ä¸‰å¤©åå¤´ç—›æœªç¼“è§£ï¼Œå»ºè®®æ›´æ¢â½Œç—›è¯ã€‚"
```

ä½¿ç”¨ConversationEntityMemory:

```json
{ 
"ç—‡çŠ¶": "å¤´ç—›", 
"â¾å‹": "140/90", 
"å½“å‰â½¤è¯": "é˜¿å¸åŒ¹æ—ï¼ˆâ½†æ•ˆï¼‰", 
"è¿‡æ•è¯ç‰©": "â»˜éœ‰ç´ " 

}
```

ä¸¤ä¸ªç±»å‹çš„å¯¹æ¯”ï¼š

| ç»´åº¦             | ConversationSummaryMemory                                    | ConversationEntityMemory                                     |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| è¾“å‡º             | è‡ªç„¶è¯­è¨€æ–‡æœ¬ï¼ˆä¸€æ®µè¯ï¼‰                                       | ç»“æ„åŒ–å­—å…¸ï¼ˆé”®å€¼å¯¹ï¼‰                                         |
| ä¸‹æ¸¸å¦‚ä½•åˆ©ç”¨ä¿¡æ¯ | éœ€å¤§æ¨¡å‹ â€œè¯»æ‡‚â€ æ‘˜è¦æ–‡æœ¬ï¼Œå¦‚æœ AI çš„æ³¨æ„åŠ›é›†ä¸­åœ¨ â€œå¤´ç—›â€ å’Œ â€œæ¢è¯â€ ä¸Šï¼Œå¯èƒ½ä¼šå¿½ç•¥è¿‡æ•æç¤ºï¼ˆå°¤å…¶æ˜¯æ‘˜è¦è¾ƒé•¿æ—¶ï¼‰ | æ— éœ€ä¾èµ–æ¨¡å‹çš„ â€œé˜…è¯»ç†è§£èƒ½åŠ›â€ï¼Œç›´æ¥é€šè¿‡å­—æ®µåï¼ˆå¦‚è¿‡æ•è¯ç‰©ï¼‰æŸ¥è¯¢ |
| é˜²é”™å¯é æ€§       | ä½ï¼ˆä¾èµ–å¤§æ¨¡å‹çš„æ³¨æ„åŠ›ï¼‰                                     | é«˜ï¼ˆé€šè¿‡ä»£ç å¼ºåˆ¶æ£€æŸ¥ï¼‰                                       |
| æ¨èå¤„ç†         | å¯ä»¥è¯•è¯•é˜¿è«è¥¿æ—ï¼ˆä¸€ç§é’éœ‰ç´ ç±»è¯ï¼‰                           | å®Œå…¨é¿å…æ¨èè¿‡æ•è¯ç‰©                                         |



### ConversationKGMemory

ConversationKGMemoryæ˜¯ä¸€ç§åŸºäº**çŸ¥è¯†å›¾è°±ï¼ˆKnowledge Graphï¼‰**çš„å¯¹è¯è®°å¿†æ¨¡å—ï¼Œå®ƒæ¯” ConversationEntityMemory æ›´è¿›ä¸€æ­¥ï¼Œä¸ä»…èƒ½è¯†åˆ«å’Œå­˜å‚¨å®ä½“ï¼Œè¿˜èƒ½æ•æ‰å®ä½“ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œå½¢æˆç»“æ„åŒ–çš„çŸ¥è¯†ç½‘ç»œã€‚ 

**ç‰¹ç‚¹ï¼š **

- çŸ¥è¯†å›¾è°±ç»“æ„ å°†å¯¹è¯å†…å®¹è½¬åŒ–ä¸º (å¤´å®ä½“, å…³ç³», å°¾å®ä½“) çš„ä¸‰å…ƒç»„å½¢å¼ 
- åŠ¨æ€å…³ç³»æ¨ç†



### VectorStoreRetrieverMemory

VectorStoreRetrieverMemoryæ˜¯ä¸€ç§åŸºäº**å‘é‡æ£€ç´¢**çš„å…ˆè¿›è®°å¿†æœºåˆ¶ï¼Œå®ƒå°†å¯¹è¯å†å²å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ï¼Œé€šè¿‡**è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢**ç›¸å…³ä¿¡æ¯ï¼Œè€Œéä¼ ç»Ÿçš„çº¿æ€§è®°å¿†æ–¹å¼ã€‚æ¯æ¬¡è°ƒç”¨æ—¶ï¼Œå°±ä¼šæŸ¥æ‰¾ä¸è¯¥è®°å¿†å…³è”æœ€é«˜çš„kä¸ªæ–‡æ¡£ã€‚ 

**é€‚ç”¨åœºæ™¯ï¼š**è¿™ç§è®°å¿†ç‰¹åˆ«é€‚åˆéœ€è¦é•¿æœŸè®°å¿†å’Œè¯­ä¹‰ç†è§£çš„å¤æ‚å¯¹è¯ç³»ç»Ÿã€‚



## Tools

è¦æ„å»ºæ›´å¼ºå¤§çš„AIå·¥ç¨‹åº”ç”¨ï¼Œåªæœ‰ç”Ÿæˆæ–‡æœ¬è¿™æ ·çš„â€œ**çº¸ä¸Šè°ˆå…µ**â€èƒ½åŠ›è‡ªç„¶æ˜¯ä¸å¤Ÿçš„ã€‚å·¥å…·Toolsä¸ä»…ä»…æ˜¯â€œè‚¢ä½“â€çš„å»¶ä¼¸ï¼Œæ›´æ˜¯ä¸ºâ€œå¤§è„‘â€æ’ä¸Šäº†æƒ³è±¡åŠ›çš„â€œç¿…è†€â€ã€‚å€ŸåŠ©å·¥å…·ï¼Œæ‰èƒ½è®©AIåº”ç”¨çš„èƒ½åŠ›çœŸæ­£å…·å¤‡æ— é™çš„å¯èƒ½ï¼Œæ‰èƒ½ä»â€œ**è®¤è¯†ä¸–ç•Œ**â€èµ°å‘â€œ**æ”¹å˜ä¸–ç•Œ**â€ã€‚

LangChain æ‹¥æœ‰å¤§é‡ç¬¬ä¸‰æ–¹å·¥å…·ã€‚è¯·è®¿é—®å·¥å…·é›†æˆæŸ¥çœ‹å¯ç”¨å·¥å…·åˆ—è¡¨ã€‚ 

https://python.langchain.com/v0.2/docs/integrations/tools/



Tools æœ¬è´¨ä¸Šæ˜¯å°è£…äº†ç‰¹å®šåŠŸèƒ½çš„å¯è°ƒç”¨æ¨¡å—ï¼Œæ˜¯Agentã€Chainæˆ–LLMå¯ä»¥ç”¨æ¥ä¸ä¸–ç•Œäº’åŠ¨çš„æ¥å£ã€‚ 

**Tool é€šå¸¸åŒ…å«å¦‚ä¸‹å‡ ä¸ªè¦ç´ ï¼š **

- name ï¼šå·¥å…·çš„åç§° 
- description ï¼šå·¥å…·çš„åŠŸèƒ½æè¿° 
- è¯¥å·¥å…·è¾“å…¥çš„ JSONæ¨¡å¼ 
- è¦è°ƒç”¨çš„å‡½æ•° 
- return_direct ï¼šæ˜¯å¦åº”å°†å·¥å…·ç»“æœç›´æ¥è¿”å›ç»™ç”¨æˆ·ï¼ˆä»…å¯¹Agentç›¸å…³ï¼‰ ï¼Œå½“return_direct=Falseæ—¶ï¼Œå·¥å…·æ‰§è¡Œç»“æœä¼šè¿”å›ç»™Agentï¼Œè®©Agentå†³å®šä¸‹ä¸€æ­¥æ“ä½œï¼›è€Œreturn_direct=Trueåˆ™ä¼šä¸­æ–­è¿™ä¸ªå¾ªç¯ï¼Œç›´æ¥ç»“æŸæµç¨‹ï¼Œè¿”å›ç»“æœç»™ç”¨æˆ·ã€‚

**å®æ“æ­¥éª¤ï¼š **

- æ­¥éª¤1ï¼šå°†nameã€description å’Œ JSONæ¨¡å¼ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™LLM 
- æ­¥éª¤2ï¼šLLMä¼šæ ¹æ®æç¤ºè¯æ¨æ–­å‡º éœ€è¦è°ƒç”¨å“ªäº›å·¥å…· ï¼Œå¹¶æä¾›å…·ä½“çš„è°ƒç”¨å‚æ•°ä¿¡æ¯ 
- æ­¥éª¤3ï¼šç”¨æˆ·éœ€è¦æ ¹æ®è¿”å›çš„å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼Œè‡ªè¡Œè§¦å‘ç›¸å…³å·¥å…·çš„å›è°ƒ



### è‡ªå®šä¹‰å·¥å…·

#### å®šä¹‰æ–¹å¼

**ç¬¬1ç§ï¼š**ä½¿ç”¨@toolè£…é¥°å™¨ï¼ˆè‡ªå®šä¹‰å·¥å…·çš„æœ€ç®€å•æ–¹å¼ï¼‰ 

è£…é¥°å™¨é»˜è®¤ä½¿ç”¨å‡½æ•°åç§°ä½œä¸ºå·¥å…·åç§°ï¼Œä½†å¯ä»¥é€šè¿‡å‚æ•°**name_or_callable**æ¥è¦†ç›–æ­¤è®¾ç½®ã€‚ 



**ç¬¬2ç§ï¼š**ä½¿ç”¨StructuredTool.from_functionç±»æ–¹æ³• 

è¿™ç±»ä¼¼äº @tool è£…é¥°å™¨ï¼Œä½†å…è®¸æ›´å¤šé…ç½®å’ŒåŒæ­¥/å¼‚æ­¥å®ç°çš„è§„èŒƒã€‚



#### å·¥å…·å®ç°

åŸºäº`tool`è£…é¥°å™¨ï¼š

```python
from langchain.tools import tool

# pydantic æ˜¯ä¸€ä¸ªç”¨äºæ•°æ®ç±»è¿›è¡Œå®šä¹‰å’ŒéªŒè¯çš„åº“
from pydantic import BaseModel, Field


# å®šä¹‰å¥½å‚æ•°åŠå‚æ•°çš„æè¿°ï¼Œæœ‰åŠ©äºå¤§æ¨¡å‹ç†è§£å‚æ•°å¹¶æ­£ç¡®çš„ç»™å‡ºè¯·æ±‚éœ€è¦çš„å‚æ•°ä½“
class FieldInfo(BaseModel):
    a: int = Field(description="ç¬¬1ä¸ªå‚æ•°")
    b: int = Field(description="ç¬¬2ä¸ªå‚æ•°")


# tool æœ‰ä¸€ä¸ªå‚æ•°æ˜¯descriptionï¼Œæœªæä¾›æ—¶ä½¿ç”¨å‡½æ•°__doc__ä¿¡æ¯ä½œä¸ºå·¥å…·æè¿°
@tool(name_or_callable="ä¸¤ä¸ªæ•´æ•°æ±‚å’Œ", args_schema=FieldInfo, return_direct=True)
def add(a: int, b: int) -> int:
    """
    ä¸¤ä¸ªæ•´æ•°ç›¸åŠ ï¼Œè¿”å›æ±‚å’Œç»“æœ
    """
    return a + b


print(f"name = {add.name}")
print(f"description = {add.description}")
print(f"args = {add.args}")
print(f"return_direct = {add.return_direct}")
"""
name = ä¸¤ä¸ªæ•´æ•°æ±‚å’Œ
description = ä¸¤ä¸ªæ•´æ•°ç›¸åŠ ï¼Œè¿”å›æ±‚å’Œç»“æœ
args = {'a': {'description': 'ç¬¬1ä¸ªå‚æ•°', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'ç¬¬2ä¸ªå‚æ•°', 'title': 'B', 'type': 'integer'}}
return_direct = True
"""

# Toolä¹Ÿæ˜¯å®ç°äº†Runnableåè®®çš„
res = add.invoke({"a": 1, "b": 2})
print(res)  # 3
```

åŸºäºStructuredToolçš„from_function()ã€‚**StructuredTool.from_function**ç±»æ–¹æ³•æä¾›äº†æ¯”**@tool**è£…é¥°å™¨æ›´å¤šçš„å¯é…ç½®æ€§ï¼Œè€Œæ— éœ€å¤ªå¤šé¢å¤–çš„ä»£ç ã€‚

```python
from langchain_core.tools import StructuredTool

# pydantic æ˜¯ä¸€ä¸ªç”¨äºæ•°æ®ç±»è¿›è¡Œå®šä¹‰å’ŒéªŒè¯çš„åº“
from pydantic import BaseModel, Field


# å®šä¹‰å¥½å‚æ•°åŠå‚æ•°çš„æè¿°ï¼Œæœ‰åŠ©äºå¤§æ¨¡å‹ç†è§£å‚æ•°å¹¶æ­£ç¡®çš„ç»™å‡ºè¯·æ±‚éœ€è¦çš„å‚æ•°ä½“
class FieldInfo(BaseModel):
    a: int = Field(description="ç¬¬1ä¸ªå‚æ•°")
    b: int = Field(description="ç¬¬2ä¸ªå‚æ•°")


def add(a: int, b: int) -> int:
    """
    ä¸¤ä¸ªæ•´æ•°ç›¸åŠ ï¼Œè¿”å›æ±‚å’Œç»“æœ
    """
    return a + b


add = StructuredTool.from_function(
    func=add,
    name="ä¸¤ä¸ªæ•´æ•°æ±‚å’Œ",
    description="ä¸¤ä¸ªæ•´æ•°ç›¸åŠ ï¼Œè¿”å›æ±‚å’Œç»“æœ",
    args_schema=FieldInfo,
    return_direct=True
)

print(f"name = {add.name}")
print(f"description = {add.description}")
print(f"args = {add.args}")
print(f"return_direct = {add.return_direct}")
"""
name = ä¸¤ä¸ªæ•´æ•°æ±‚å’Œ
description = ä¸¤ä¸ªæ•´æ•°ç›¸åŠ ï¼Œè¿”å›æ±‚å’Œç»“æœ
args = {'a': {'description': 'ç¬¬1ä¸ªå‚æ•°', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'ç¬¬2ä¸ªå‚æ•°', 'title': 'B', 'type': 'integer'}}
return_direct = True
"""

res = add.invoke({"a": 1, "b": 2})
print(res)  # 3
```



### å·¥å…·è°ƒç”¨

ç®€å•æ¥è¯´ï¼Œå·¥å…·çš„è°ƒç”¨å¹¶ä¸æ˜¯å¤§æ¨¡å‹ç›´æ¥è°ƒç”¨å·¥å…·ï¼Œè€Œæ˜¯å¤§æ¨¡å‹æ ¹æ®ä¸Šä¸‹æ–‡å’Œç”¨æˆ·è¯‰æ±‚ï¼Œç»“åˆæä¾›çš„å¯è°ƒç”¨å‡½æ•°ï¼Œè®©å¤§æ¨¡å‹å†³ç­–æ˜¯å¦éœ€è¦ä½¿ç”¨æŸä¸ªå·¥å…·ï¼Œç„¶åè¿”å›è°ƒç”¨å·¥å…·çš„åç§°ä»¥åŠå·¥å…·è°ƒç”¨æ—¶éœ€è¦çš„å‚æ•°ã€‚

```python
import os
import dotenv

from langchain_community.tools import MoveFileTool
from langchain_core.messages import HumanMessage

from langchain_core.utils.function_calling import convert_to_openai_function
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

model = ChatOpenAI(api_key=os.getenv("API_KEY"), base_url=os.getenv("API_BASE"),
                   model="gpt-4o-mini")

# å®šä¹‰å·¥å…·ã€‚æ­¤å¤„ä½¿ç”¨langchainæä¾›çš„å·¥å…·
tools = [MoveFileTool()]

# å·¥å…·è¿˜ä¸èƒ½ç›´æ¥ä½¿ç”¨ï¼Œè¦è½¬æˆå‡½æ•°åˆ—è¡¨
# è¿™é‡Œçš„ä½œç”¨å°±æ˜¯æŠŠå·¥å…·è½¬æˆå¸¦æœ‰åç§°ã€æè¿°ã€å‚æ•°ç­‰ä¿¡æ¯çš„å‡½æ•°è¯´æ˜åˆ—è¡¨ã€‚è®©å¤§æ¨¡å‹æ¥é€‰æ‹©åˆé€‚çš„å·¥å…·å¹¶è¿”å›å·¥å…·æ‰§è¡Œæ—¶éœ€è¦ä½¿ç”¨çš„å‚æ•°
functions = [convert_to_openai_function(tool) for tool in tools]
print(functions)
"""
[{'name': 'move_file', 'description': 'Move or rename a file from one location to another', 
'parameters': {
'properties': {'source_path': {'description': 'Path of the file to move', 'type': 'string'}, 
                'destination_path': {'description': 'New path for the moved file', 'type': 'string'}}, 
'required': ['source_path', 'destination_path'], 'type': 'object'}}]
"""

# æ¨¡å‹æ ¹æ®è¿°æ±‚å¾ç¨‹è¯·æ±‚ç»“æœ
resp = model.invoke(input=[HumanMessage(content="å°†æ–‡ä»¶aaa.txtç§»åŠ¨åˆ°æ¡Œé¢")], functions=functions)
print(resp.content)  #
print(resp.additional_kwargs)
"""
{'function_call': {
'arguments': '{"source_path":"aaa.txt","destination_path":"/Users/YourUsername/Desktop/aaa.txt"}', 
'name': 'move_file'
}, 'refusal': None}
"""

```

åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œcontentæ˜¯æ²¡æœ‰è¾“å‡ºçš„ï¼Œå› ä¸ºå¤§æ¨¡å‹åˆ¤æ–­éœ€è¦è°ƒç”¨å·¥å…·ã€‚å¦‚æœå¤§æ¨¡å‹åˆ¤æ–­ä¸éœ€è¦è°ƒç”¨å·¥å…·ï¼Œé‚£ä¹ˆcontentå°±ä¼šæœ‰ç»“æœï¼Œä½†æ˜¯ additional_kwargs ä¸­å°±ä¸ä¼šæœ‰function_call äº†ã€‚

æ‰€ä»¥ï¼Œåœ¨å¾—åˆ°å¤§æ¨¡å‹ç»“æœåï¼Œæˆ‘ä»¬éœ€è¦åˆ¤æ–­æ˜¯å¦è¦è¿›è¡Œå·¥å…·æ‰§è¡Œï¼Œä¸‹é¢ä»¥åˆ é™¤æ–‡ä»¶ä¸ºä¾‹ï¼š

```python
import json
import os
import dotenv

from langchain_community.tools import DeleteFileTool
from langchain_core.messages import HumanMessage

from langchain_core.utils.function_calling import convert_to_openai_function
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

model = ChatOpenAI(api_key=os.getenv("API_KEY"), base_url=os.getenv("API_BASE"),
                   model="gpt-4o-mini")

# å®šä¹‰å·¥å…·ã€‚æå‰è®°å½•åå­—å’Œå·¥å…·çš„æ˜ å°„
tools = [DeleteFileTool()]
tools_map = {tool.name: tool for tool in tools}
functions = [convert_to_openai_function(tool) for tool in tools]

# æ¨¡å‹æ ¹æ®è¿°æ±‚å¾ç¨‹è¯·æ±‚ç»“æœ
resp = model.invoke(input=[HumanMessage(content="å°†æ–‡ä»¶aaa.txtåˆ æ‰")], functions=functions)

if "function_call" in resp.additional_kwargs:
    func_call = resp.additional_kwargs["function_call"]
    arguments = func_call["arguments"]
    func = tools_map.get(func_call.get("name", ""))
    if func:
        result = func.run(json.loads(arguments))
        print(func.name + " å·¥å…·è°ƒç”¨æˆåŠŸ")
    else:
        print("æ— æ•ˆçš„å·¥å…·åç§°: " + func_call.get("name", ""))
else:
    print(resp.content)
```



