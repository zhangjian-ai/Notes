## 一元线性回归

一元线性回归是分析只有一个自变量（自变量x 和 因变量y）线性相关关系的方法，是计算机领域中最为简单的预测模型，也是最重要的基础。一元线性回归的公式为：
$$
f(x)=y=ax+b
\\
a 代表权重（也称 斜率）；b 代表偏置bias（即x为0时，在y轴上的截距）
$$
一元线性回归的核心目的，就是求解 a 和 b 的值，确定这两个值之后，就可以预测下一个x多对应的y值



### 提出问题

根据比萨斜塔的倾斜度历史数据，预测未来的倾斜度。

| 年份   | 75   | 76   | 77   | 78   | 79   | 80   | 81   | 82   | 83   | 84   | 85   | 86   | 87   |
| :----- | ---- | ---- | ---- | :--- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 倾斜度 | 640  | 645  | 660  | 661  | 673  | 688  | 696  | 710  | 726  | 727  | 740  | 742  | 757  |

我们使用`matplotlob`库，绘制一个散点图，观察一些倾斜度和年份之间的关系。

代码：

```python
import matplotlib.pyplot as plt

x = [75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]
y = [640, 645, 660, 661, 673, 688, 696, 710, 726, 727, 740, 742, 757]

plt.scatter(x, y)
plt.show()
```

散点图：

<img src="./images/linear_regression/001.png" style="float: left;width: 55%">

从图中可以看出，倾斜度的变化比较接近于一条直线，但并非直线。所以，想要预测88年的倾斜度，就可以使用一元线性回归进行处理。



### 方差和标准差

对于一组数据来说，方差是将各个变量与其均值的差求平方后再求和的平均数。它反映了样本中各个观测点到其均值的平均离散程度。

方差（Variance）的公式如下：

$$
\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (X_i - \mu)^2
\\
其中：
\sigma^2 表示方差，N 是样本的总数，X_i 是第 i 个数据点的值，\mu 是总体的均值（计算公式：\mu = \frac{1}{N} \sum_{i=1}^{N} X_i）
$$

标准差（Standard Deviation），是在方差的基础上开根，公示如下：
$$
\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (X_i - \mu)^2}
\\
其中：
\sigma 表示标准差，N 是样本的总数，X_i 是第 i 个数据点的值，\mu 是总体的均值（计算公式：\mu = \frac{1}{N} \sum_{i=1}^{N} X_i）
$$
我们使用numpy来计算一下比萨斜塔倾斜度的方差与标准差：

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.array([75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87])
y = np.array([640, 645, 660, 661, 673, 688, 696, 710, 726, 727, 740, 742, 757])

# 方差
print(np.var(y))  # 1435.2899408284025

# 标准差
print(np.std(y))  # 37.885220612112086
```



标准差越小，说明各个数据的差异越小（比如极端情况下，所有观测值相等，那么方差和标准差都为0），反之则差异越大。那么如何评估 37.885 在样本中的差异大小呢，可以用 标准差/样本平均值 ，比例越小，差异越小：

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.array([75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87])
y = np.array([640, 645, 660, 661, 673, 688, 696, 710, 726, 727, 740, 742, 757])

# 5.4% 算是相对比较小的差异
print(np.std(y) / np.mean(y))  # 0.05433070799310062
```



### 相关系数

相关系数（r）显示了一个线性关系的强度和方向（正或负）。当两个变量之间呈现正相关时，r 为整数；当两个变量之间呈负相关时，r 也为负数。如果数据点正好描述了一条直线，r 等于 1 或 -1；如果数据点完全没有相关关系时，r 等于零。如果某组数据的相关系数相当低（0.5>r>-0.5），那么线性回归可能不会给我们带来非常可信的结果，只有当 r>0.5 或 r < -0.5 时，才值得做线性回归。

<img src="./images/linear_regression/002.png" style="float: left;width: 55%">

相关系数 r 的公式为：
$$
r = \frac{\sum_{i=1}^{n} (x_i - \overline{x})*(y_i - \overline{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \overline{x})^2 * \sum_{i=1}^{n} (y_i - \overline{y})^2}}
\\
* 带上划线的变量表示样本平均值 *
$$
当前问题中，年份数据为自变量x，倾斜度为因变量y，将前面的数据带入计算得到 r = 0.99468。

下面使用numpy在计算一次，验证是否正确：

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.array([75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87])
y = np.array([640, 645, 660, 661, 673, 688, 696, 710, 726, 727, 740, 742, 757])

print(np.corrcoef(x, y))
"""
[[1.         0.99468433]
 [0.99468433 1.        ]]
"""
"""
返回一个二维数组
第一个子数组两个值分别是 x和x 的相关系数，x和y的相关系数
第二个子数组两个值分别是 y和x 的相关系数，y和y的相关系数
"""

# x和y的相关系数
r = np.corrcoef(x, y)[0, 1]

print(r)  # 0.994684325465011
```

从相关系数 r 的值可以看出，当前的数据样本，是非常适合使用线性回归来预测结果的。



### 最小二乘法

截止目前，我们使用标准差评估了样本的离散程度很小，使用相关系数分析了样本线性相关程度很高。但还是没有求出 a、b 的值。我们希望找到一个数字 a 和 b，使得绘制出的一条直线与倾斜度的点的距离越小越好，即满足以下公式：
$$
d_i=\sum_{i=1}^{n}(y_i-\hat{y_i})^2
\\
y_i表示年份对应的倾斜度;\hat{y_i}表示绘制的直线在所在年份的值
$$
当`di`的值最小时，就是最小二乘法。最小二乘法的公式为（S 表示 标准差）：
$$
a=r*\frac{S_y}{S_x}
\\
b=\overline{y}-a*\overline{x}
$$
使用python代码来计算 a 和 b 的值：

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.array([75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87])
y = np.array([640, 645, 660, 661, 673, 688, 696, 710, 726, 727, 740, 742, 757])

# x和y的相关系数
r = np.corrcoef(x, y)[0, 1]

# y的标准差、平均值
Sy = np.std(y)
My = np.mean(y)

# x的标准差、平均值
Sx = np.std(x)
Mx = np.mean(x)

# 得出 a b 的值
a = r * (Sy / Sx)
b = My - a * Mx

print(a)  # 10.071428571428571
print(b)  # -118.47802197802196
```

截取小数后两位，得出一元线性回归函数：
$$
y=10.07*x-118.48
$$
由此可以预测出88年比萨斜塔的倾斜度为：
$$
y=10.07*88-118.48=767.68
$$
基于求得a、b值，也可以把散点图和直线图都会指出来，来直观的感受一下。

代码：

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.array([75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87])
y = np.array([640, 645, 660, 661, 673, 688, 696, 710, 726, 727, 740, 742, 757])

plt.scatter(x, y)
plt.plot(x, 10.07*x-118.48, color="red")
plt.show()
```

绘制图：

<img src="./images/linear_regression/003.png" style="float: left;width: 55%">



### SKLearn 求解

上面我们使用最小二乘法的公式，直接计算出了函数的 斜率 和 偏置，我们也可以直接使用 SKLearn 机器学习库来完成线性回归函数的拟合。

```python
import numpy as np

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# 使用模型训练时，要求数据是二维的，因此这里转为 13行1列 的数据
x = np.array([75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]).reshape(-1, 1)
y = np.array([640, 645, 660, 661, 673, 688, 696, 710, 726, 727, 740, 742, 757]).reshape(-1, 1)

# 准备训练集和测试集。test_size 表示测试集占整个数据集的比例
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

# 训练模型
model = LinearRegression()
model.fit(x_train, y_train)

# 输出斜率和偏置
print(model.coef_)  # [[10.0244898]]
print(model.intercept_)  # [-114.99591837]

# 使用测试集计算准确率
print(model.score(x_test, y_test))  # 0.995850645830478

# 预测88年的数据。预测数据维度要和训练数据保持一致
print(model.predict(np.array([[88]])))  # [[767.15918367]]
```

分析：

1. 最终预测结果，与直接使用最小二乘法有一点偏差
2. 拟合出的 斜率 和 偏置 也和最小二乘法有一些差距，是因为我们使用的训练数据集其实并不是全部数据，有20%留作测试集了，如果直接适用所有数据拟合，最终预测出的结果和最小二乘法的没差别



## 一元线性回归实战

### 准备工作









